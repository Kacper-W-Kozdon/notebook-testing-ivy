{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":36545,"sourceType":"datasetVersion","datasetId":1309}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kacperkodo/sarcasm-detection-using-the-ivy-library?scriptVersionId=172714662\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# DEPENDANCIES AND SETUP","metadata":{"id":"s2B-C0ETR8j-"}},{"cell_type":"markdown","source":"Installing kaggle and uploading the API key necessary to use it.","metadata":{"id":"lVY3Z4myS1O4"}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"id":"GRXT-eQOLn0q","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q kaggle\n# from google.colab import files\n# from google.colab import userdata\nimport os\n# files.upload(); #Upload kaggle.json - you can get from the kaggle account settings, from the API section.","metadata":{"id":"7R4luV8tSDFn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# UNCOMMENT BELOW IF YOU'RE RUNNING THE NOTEBOOK OUTSIDE KAGGLE\n\n# kaggle_api_key = open('kaggle.json', \"w+\")\n# kaggle_api_key.write('<this is where you copy the contents of your kaggle.json>') # kaggle.json - you can get it from the kaggle account settings, from the API section.\n# !mkdir ~/.kaggle\n# !cp kaggle.json ~/.kaggle/\n# !chmod 600 ~/.kaggle/kaggle.json\n# !kaggle datasets list","metadata":{"id":"F39aDC9VLn0u","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Installing packages necessary to use torch's transformers.","metadata":{"id":"GRyxyRkNqONt"}},{"cell_type":"code","source":"!pip install tqdm boto3 requests regex sentencepiece sacremoses botocore>=1.34.79","metadata":{"id":"yhD653HGqOj2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To use the API, credentials need to be copied into the kaggle folder. If everything works, the output will show the list of available datasets.","metadata":{"id":"aCN2c1DGTbVM"}},{"cell_type":"markdown","source":"import json","metadata":{"id":"P1aQHs-9Tkt2"}},{"cell_type":"code","source":"import json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Preparing the ivy library.","metadata":{"id":"_Sf8EImZT6kZ"}},{"cell_type":"code","source":"#Insert the correct user when cloning the repos. Make sure that they are up-to-date.\n\n!git clone \"https://github.com/Kacper-W-Kozdon/demos.git\"\n# !git clone \"https://github.com/Kacper-W-Kozdon/ivy.git\"\n!pip install -U -q paddlepaddle ivy accelerate>=0.21.0  2>/dev/null # If ran in a notebook with only cpu enabled, edit \"paddlepaddle-gpu\" to \"paddlepaddle\"","metadata":{"id":"7DMn3EoEUBGQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next: import the ivy library and get the dataset.","metadata":{"id":"y1sA3gFuWjDE"}},{"cell_type":"code","source":"import ivy","metadata":{"id":"_NUgteS_Dluc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import the libraries suggested in the model which is to be transpiled.","metadata":{"id":"UKN-VX8QXDEG"}},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd  # For data manipulation and analysis\nimport gc  # For garbage collection to manage memory\nimport re  # For regular expressions\nimport numpy as np  # For numerical operations and arrays\nimport tensorflow as tf\nimport torch  # PyTorch library for deep learning\nimport paddle","metadata":{"id":"1-LVDQOELn0x","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Libraries to accompany torch's transformers\nimport tqdm\nimport boto3\nimport requests\nimport regex\nimport sentencepiece\nimport sacremoses\n\nimport warnings  # For handling warnings\nwarnings.filterwarnings(\"ignore\")  # Ignore warning messages\n\nfrom transformers import AutoModel, AutoTokenizer  # Transformers library for natural language processing\n# from transformers import TextDataset, LineByLineTextDataset, DataCollatorForLanguageModeling, \\\n# pipeline, Trainer, TrainingArguments, DataCollatorWithPadding  # Transformers components for text processing\nfrom transformers import TextDataset, LineByLineTextDataset, DataCollatorForLanguageModeling, \\\npipeline, TrainingArguments, DataCollatorWithPadding\nfrom transformers import AutoModelForSequenceClassification  # Transformer model for sequence classification\n\nimport accelerate\n\n# from nlp import Dataset  # Import custom 'Dataset' class for natural language processing tasks\nfrom imblearn.over_sampling import RandomOverSampler  # For oversampling to handle class imbalance\n# import datasets  # Import datasets library\n# from datasets import Dataset, Image, ClassLabel  # Import custom 'Dataset', 'ClassLabel', and 'Image' classes\nfrom transformers import pipeline  # Transformers library for pipelines\nfrom bs4 import BeautifulSoup  # For parsing HTML content\n\nimport matplotlib.pyplot as plt  # For data visualization\nimport itertools  # For working with iterators\nfrom sklearn.metrics import (  # Import various metrics from scikit-learn\n    accuracy_score,  # For calculating accuracy\n    roc_auc_score,  # For ROC AUC score\n    confusion_matrix,  # For confusion matrix\n    classification_report,  # For classification report\n    f1_score  # For F1 score\n)\n\n# from datasets import load_metric  # Import load_metric function to load evaluation metrics\n\nfrom tqdm import tqdm  # For displaying progress bars\n\ntqdm.pandas()  # Enable progress bars for pandas operations","metadata":{"id":"19rgBXHJXHFu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"gpu:0\" if torch.cuda.is_available() else \"cpu\"\n# This line checks if a CUDA-enabled GPU is available.\n# If a GPU is available, it sets the device to \"gpu:0\" (the first GPU).\n# If no GPU is available, it sets the device to \"cpu\".\n\nivy.set_default_device(device)\n# This line sets the default device for Ivy operations.\n# Ivy will use the device specified above (either \"gpu:0\" or \"cpu\") for all computations.\n\nivy.set_soft_device_mode(True)\n# This line enables Ivy's \"soft device mode\".\n# In soft device mode, Ivy will attempt to automatically move tensors to the correct device\n# when performing operations involving tensors on different devices.\n# This can simplify tensor management and device handling in certain cases.\n","metadata":{"id":"bXr9tGFLGRPI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(ivy.default_device())\n# This line prints the current default device set in Ivy.\n# It will print either \"gpu:0\" or \"cpu\", depending on the device specified earlier.\n\nprint(ivy.num_gpus())\n# This line prints the number of CUDA-enabled GPUs available on the system.\n# If one or more GPUs are available, it will print the number of GPUs.\n# If no GPU is available, it will print 0.\n\nprint(torch.cuda.is_available())\n# This line checks if PyTorch can access a CUDA-enabled GPU.\n# PyTorch is a popular machine learning library, and it uses CUDA for GPU acceleration.\n# If a CUDA-enabled GPU is available and PyTorch can access it, this line will print True.\n# If no CUDA-enabled GPU is available or PyTorch cannot access it, this line will print False.","metadata":{"id":"ijs6fSKL9QZ4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Set the seeds.","metadata":{"id":"JU7qbxYdsVlK"}},{"cell_type":"code","source":"tf.keras.utils.set_random_seed(0)\n# This line sets a seed value for the random number generator used by TensorFlow and Keras.\n# Setting a seed allows you to reproduce the same results across different runs.\n# In this case, the seed is set to 0, which is a common value used for reproducibility.\n\ntorch.manual_seed(0)\n# This line sets the seed for the random number generator used by PyTorch.\n# Similar to the TensorFlow seed, it helps ensure reproducibility of results.\n# The seed value is set to 0 here.\n\npaddle.seed(0)\n# This line sets the seed for the random number generator used by PaddlePaddle.\n# PaddlePaddle is another deep learning framework, and setting the seed ensures reproducibility.\n# Again, the seed value is set to 0 for consistency across frameworks.","metadata":{"id":"HxD1xridsU_l","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get the API key for ivy transpiler from your account and upload it to the project. Move it to the correct directory.","metadata":{"id":"zwU4oNrkXyxT"}},{"cell_type":"code","source":"pwd","metadata":{"id":"VT4kcIOKLn00","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First we're loading the tokenizer and the model from torch. All of the basic set-up instructions can be found here: https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/huggingface_pytorch-transformers.ipynb#scrollTo=72d8f2de","metadata":{"id":"DWCnfosUshGK"}},{"cell_type":"code","source":"tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\n","metadata":{"id":"2rZY3rhisgXZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import ivy libraries\n\nfrom ivy.stateful.module import Module\nfrom ivy.stateful.sequential import Sequential\nfrom ivy.stateful.layers import *\nfrom ivy.stateful.losses import *\nfrom ivy.stateful.optimizers import *\nfrom ivy.stateful.activations import *\nfrom ivy.stateful.initializers import *\nfrom ivy.stateful.norms import *\n","metadata":{"id":"b0MdcERZYS_6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importing dataset and preprocessing\ndf = pd.read_csv(\"/kaggle/input/sarcasm/train-balanced-sarcasm.csv\")\ndf = df.drop_duplicates()\ndf = df.rename(columns={'comment': 'title'})\ndf = df[['label', 'title']]\ndf = df[~df['label'].isnull()]\ndf = df[~df['title'].isnull()]\ndf.sample(5)","metadata":{"id":"YmocEJ3ALn00","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATASET AND MODEL OVERVIEW","metadata":{"id":"bXFPiT6SgPob"}},{"cell_type":"code","source":"!echo -n API_KEY > .ivy/key.pem","metadata":{"id":"cXATe1TGLn01","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_words(text: str) -> int:\n    return len(text.split())\n\ndef count_symbols(text: str) -> int:\n    return len(\"\".join(text.split()))\n\ndef symbol_to_word_ratio(text: str) -> float:\n    return count_symbols(text)/count_words(text)\n\ndef upper_lower_ratio(text: str) -> float:\n    text = \"\".join(text.split())\n    return sum(1 for c in text if c.isupper())/(max([sum(1 for c in text if c.islower()), 1]))\n\ndf['word_count'] = df[\"title\"].apply(count_words)\ndf['symbol_count'] = df[\"title\"].apply(count_symbols)\ndf[\"upper_lower_ratio\"] = df[\"title\"].apply(upper_lower_ratio)\ndf[\"symbol_to_word_ratio\"] = df[\"title\"].apply(symbol_to_word_ratio)\ndf.sample(5)","metadata":{"id":"BVA6U5Y0c7vg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A few plots to see some some characteristics of the data.","metadata":{"id":"YY9ru5DbJqxa"}},{"cell_type":"code","source":"df_no_sarc = df.where(df[\"label\"] == 0)\ndf_no_sarc = df_no_sarc.where(df_no_sarc[\"word_count\"] <= 51)\ndf_sarc = df.where(df[\"label\"] == 1)\ndf_sarc = df_sarc.where(df_sarc[\"word_count\"] <= 51)\ndf_no_sarc = df_no_sarc[np.isfinite(df_no_sarc[\"word_count\"])]\ndf_sarc = df_sarc[np.isfinite(df_sarc[\"word_count\"])]\nplt.style.use('_mpl-gallery-nogrid')\n\nhist_df_no_sarc, bin_edges_no = np.histogram(df_no_sarc[\"word_count\"].values, density=True)\nhist_df_sarc, bin_edges = np.histogram(df_sarc[\"word_count\"].values, density=True)\n# plot:\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nbin_mids_no = [(bin_edges_no[i+1] + bin_edges_no[i])/2 for i in range(len(bin_edges_no) - 1)]\nbin_mids = [(bin_edges[i+1] + bin_edges[i])/2 for i in range(len(bin_edges) - 1)]\nax1.bar(bin_mids_no, hist_df_no_sarc, width=bin_edges_no[1] - bin_edges_no[0])\nax2.bar(bin_mids, hist_df_sarc, width=bin_edges[1] - bin_edges[0])\nax1.set_title(\"Hist no sarcasm\")\nax1.set_ylabel(\"density\")\nax1.set_xlabel(\"word count\")\nax1.set_xticks(bin_edges_no)\nax1.grid(True)\nax2.set_title(\"Hist sarcasm\")\nax2.set_xlabel(\"word count\")\nax2.set_xticks(bin_edges)\nax2.grid(True)\nplt.show()","metadata":{"id":"_LEHSFedgIBq","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_no_sarc = df.where(df[\"label\"] == 0)\ndf_no_sarc = df_no_sarc.where(df_no_sarc[\"symbol_count\"] <= 201)\ndf_sarc = df.where(df[\"label\"] == 1)\ndf_sarc = df_sarc.where(df_sarc[\"symbol_count\"] <= 201)\ndf_no_sarc = df_no_sarc[np.isfinite(df_no_sarc[\"symbol_count\"])]\ndf_sarc = df_sarc[np.isfinite(df_sarc[\"symbol_count\"])]\nplt.style.use('_mpl-gallery-nogrid')\n\nhist_df_no_sarc, bin_edges_no = np.histogram(df_no_sarc[\"symbol_count\"].values, density=True)\nhist_df_sarc, bin_edges = np.histogram(df_sarc[\"symbol_count\"].values, density=True)\n# plot:\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nbin_mids_no = [(bin_edges_no[i+1] + bin_edges_no[i])/2 for i in range(len(bin_edges_no) - 1)]\nbin_mids = [(bin_edges[i+1] + bin_edges[i])/2 for i in range(len(bin_edges) - 1)]\nax1.bar(bin_mids_no, hist_df_no_sarc, width=bin_edges_no[1] - bin_edges_no[0])\nax2.bar(bin_mids, hist_df_sarc, width=bin_edges[1] - bin_edges[0])\nax1.set_title(\"Hist no sarcasm\")\nax1.set_ylabel(\"density\")\nax1.set_xlabel(\"symbol count\")\nax1.set_xticks(bin_edges_no)\nax1.grid(True)\nax2.set_title(\"Hist sarcasm\")\nax2.set_xlabel(\"symbol count\")\nax2.set_xticks(bin_edges)\nax2.grid(True)\nplt.show()","metadata":{"id":"RcYhYzfygLc9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_no_sarc = df.where(df[\"label\"] == 0)\ndf_no_sarc = df_no_sarc.where(df_no_sarc[\"upper_lower_ratio\"] <= 0.3)\ndf_sarc = df.where(df[\"label\"] == 1)\ndf_sarc = df_sarc.where(df_sarc[\"upper_lower_ratio\"] <= 0.3)\ndf_no_sarc = df_no_sarc[np.isfinite(df_no_sarc[\"upper_lower_ratio\"])]\ndf_sarc = df_sarc[np.isfinite(df_sarc[\"upper_lower_ratio\"])]\nplt.style.use('_mpl-gallery-nogrid')\n\nhist_df_no_sarc, bin_edges_no = np.histogram(df_no_sarc[\"upper_lower_ratio\"].values, density=True)\nhist_df_sarc, bin_edges = np.histogram(df_sarc[\"upper_lower_ratio\"].values, density=True)\n# plot:\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nbin_mids_no = [(bin_edges_no[i+1] + bin_edges_no[i])/2 for i in range(len(bin_edges_no) - 1)]\nbin_mids = [(bin_edges[i+1] + bin_edges[i])/2 for i in range(len(bin_edges) - 1)]\nax1.bar(bin_mids_no, hist_df_no_sarc, width=bin_edges_no[1] - bin_edges_no[0])\nax2.bar(bin_mids, hist_df_sarc, width=bin_edges[1] - bin_edges[0])\nax1.set_title(\"Hist no sarcasm\")\nax1.set_ylabel(\"density\")\nax1.set_xlabel(\"upper/lower ratio\")\nax1.set_xticks(bin_edges_no)\nax1.grid(True)\nax2.set_title(\"Hist sarcasm\")\nax2.set_xlabel(\"upper/lower ratio\")\nax2.set_xticks(bin_edges)\nax2.grid(True)\nplt.show()","metadata":{"id":"vvKbuhLaDaSP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_no_sarc = df.where(df[\"label\"] == 0)\ndf_no_sarc = df_no_sarc.where(df_no_sarc[\"symbol_to_word_ratio\"] <= 11)\ndf_sarc = df.where(df[\"label\"] == 1)\ndf_sarc = df_sarc.where(df_sarc[\"symbol_to_word_ratio\"] <= 11)\ndf_no_sarc = df_no_sarc[np.isfinite(df_no_sarc[\"symbol_to_word_ratio\"])]\ndf_sarc = df_sarc[np.isfinite(df_sarc[\"symbol_to_word_ratio\"])]\nplt.style.use('_mpl-gallery-nogrid')\n\nhist_df_no_sarc, bin_edges_no = np.histogram(df_no_sarc[\"symbol_to_word_ratio\"].values, density=True)\nhist_df_sarc, bin_edges = np.histogram(df_sarc[\"symbol_to_word_ratio\"].values, density=True)\n# plot:\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nbin_mids_no = [(bin_edges_no[i+1] + bin_edges_no[i])/2 for i in range(len(bin_edges_no) - 1)]\nbin_mids = [(bin_edges[i+1] + bin_edges[i])/2 for i in range(len(bin_edges) - 1)]\nax1.bar(bin_mids_no, hist_df_no_sarc, width=bin_edges_no[1] - bin_edges_no[0])\nax2.bar(bin_mids, hist_df_sarc, width=bin_edges[1] - bin_edges[0])\nax1.set_title(\"Hist no sarcasm\")\nax1.set_ylabel(\"density\")\nax1.set_xlabel(\"symbols/words ratio\")\nax1.set_xticks(bin_edges_no)\nax1.grid(True)\nax2.set_title(\"Hist sarcasm\")\nax2.set_xlabel(\"symbols/words ratio\")\nax2.set_xticks(bin_edges)\nax2.grid(True)\nplt.show()","metadata":{"id":"HkuIzb1JF1U1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"id":"nk3vK1u3FlZc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BUILDING LSTM ON CORE IVY","metadata":{"id":"Vy7iZ8QnAS2-"}},{"cell_type":"code","source":"# dir(tokenizer)","metadata":{"id":"SEPdRlMT6Ybd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Setting up the device for the computations.","metadata":{"id":"hKaKQ1kRLn04"}},{"cell_type":"code","source":"# imports the cuDF library (CUDA Data Frame) into your Python environment. Comment out if running on CPU.\nimport cudf","metadata":{"id":"RsojtOQNLn04","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_test_ratio = 0.9\nfrac_dataset = 0.02","metadata":{"id":"oj86qS4aLn05","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importing dataset and preprocessing\ndf = cudf.read_csv(\"/kaggle/input/sarcasm/train-balanced-sarcasm.csv\") \n# df = pd.read_csv(\"/kaggle/input/sarcasm/train-balanced-sarcasm.csv\") # Comment above and uncomment this line if running on CPU.\ndf = df.drop_duplicates()\ndf = df.rename(columns={'comment': 'title'})\ndf = df[['label', 'title']]\ndf = df[~df['label'].isnull()]\ndf = df[~df['title'].isnull()]\ndf.sample(frac=1).reset_index(drop=True)\ndf.sample(5)\n","metadata":{"id":"2ft2bHc1Ln05","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_full = df # Create a copy of the original dataframe\ndf_size = len(df_full) # Get the total number of rows in the dataframe\nsplit = int(df_size * train_test_ratio * frac_dataset) # Calculate the split index for the training set\ncutoff = int(df_size * frac_dataset) # Calculate the cutoff index for the evaluation set\n\n# Split the dataframe into training and evaluation sets\ndf = df_full.iloc[:split,:] # Training set\ndf_eval = df_full.iloc[split:cutoff,:] # Evaluation set\n\nprint(len(df)) # Print the number of rows in the training set","metadata":{"id":"CSTVRW2DLn05","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(torch.cuda.is_available())\n# Check if PyTorch can access a CUDA-enabled GPU\n\ndevice = \"cpu\" if not torch.cuda.is_available() else ivy.as_native_dev(\"gpu:0\")\n# Get the Ivy device object for \"gpu:0\" (the first CUDA-enabled GPU)\n\nivy.set_default_device(device)\n# Set the default device in Ivy to \"gpu:0\"\n\nprint(ivy.default_device())\n# Print the current default device in Ivy\n\nivy.set_soft_device_mode(True)\n# Enable Ivy's soft device mode, which automatically moves tensors to the correct device\n\nprint(device)\n# Print the Ivy device object for \"gpu:0\"","metadata":{"id":"VQlQ4V_-Ln06","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.vocab_size)\n# Print the size of the vocabulary (number of unique tokens) used by the tokenizer\n\nprint(tokenizer.all_special_tokens_extended)\n# Print a list of all special tokens (e.g., padding token, start/end of sequence tokens) used by the tokenizer, including those added by extensions\n\nprint(tokenizer.all_special_ids)\n# Print a list of token IDs corresponding to the special tokens used by the tokenizer\n\nprint(tokenizer.pad_token_id)\n# Print the token ID assigned to the padding token used by the tokenizer for padding sequences to a fixed length","metadata":{"id":"mKyWQmUR6grC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = list(df.sample(8)[\"title\"].to_pandas())\n# Sample 8 rows from the \"title\" column of the DataFrame df, convert the result to a pandas Series, and store the values in the list sample\n\nprint(sample)\n# Print the sampled titles\n\ntokenizer(sample, add_special_tokens=True, padding=True, truncation=True)\n# Apply the tokenizer to the sampled titles","metadata":{"id":"_jEnXjjUOo7f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the Dataset class from the torch.utils.data module.\nfrom torch.utils.data import Dataset","metadata":{"id":"PU8tcMRKLn06","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\n\n# Define a custom dataset class named 'ivy_Dataset'.\nclass ivy_Dataset(Dataset):\n    def __init__(self, df):\n        # Initialize the dataset with the given dataframe 'df'.\n        # Store the number of samples in the dataset.\n        self.num_samples = df['title'].size\n        # Extract 'title' and 'label' from the dataframe and store as data.\n        self.data = [[entry[0], entry[1]] for entry in zip(df[\"title\"].to_pandas(), df[\"label\"].to_pandas())]\n\n    def __getitem__(self, idx):\n        # Retrieve and return the data at the given index 'idx'.\n        title = self.data[idx][0]\n        label = self.data[idx][1]\n        return title, label\n\n    def __len__(self):\n        # Return the total number of samples in the dataset.\n        return self.num_samples\n\n\n","metadata":{"id":"JEW-0sYgLn06","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data = ivy_Dataset(df)# Access the label of the 10th data sample in the 'data_sample' list.\n","metadata":{"id":"3kEn6ka1Ln07","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Randomly sample 10 entries from the dataframe 'df' and store it as 'df_sample'.\ndf_sample = df.sample(10)\n\n# Extract 'title' and 'label' from the sampled dataframe and store as 'data_sample'.\ndata_sample = [[entry[0], entry[1]] for entry in zip(df_sample[\"title\"].to_pandas(), df_sample[\"label\"].to_pandas())]\n\n# Access the label of the 10th data sample in the 'data_sample' list.\ndata_sample[9][1]","metadata":{"id":"qzV0zScmLn07","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the DataLoader class from the torch.utils.data module.\nfrom torch.utils.data import DataLoader\n\n# Create a DataLoader instance named 'train_dataloader' for the training data.\n# Set the batch size to 'batch_size', shuffle the data during each epoch.\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n","metadata":{"id":"lupRA9BvLn07","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ivy_train_loader(dataset=df, batch_size=4):\n    # Calculate the number of batches based on the dataset size and batch size.\n    num_batches = int(len(dataset) / batch_size)\n\n    # Generate batches of data using list comprehension.\n    out = (\n        (\n            dataset[\"title\"][batch_idx * batch_size : batch_idx * batch_size + batch_size].to_pandas(),\n            dataset[\"label\"][batch_idx * batch_size : batch_idx * batch_size + batch_size].to_pandas()\n        )\n        for batch_idx in range(num_batches)\n    )\n\n    return out\n\n# Create a loader using the ivy_train_loader function with the specified batch size.\nloader = ivy_train_loader(batch_size=batch_size)\n\n# Iterate over the loader with tqdm for progress tracking.\nfor batch_id, data in tqdm(enumerate(loader)):\n    # Extract input and target data from the batch.\n    x_data = data[0]\n    y_data = data[1]\n\n    # Print additional information or perform computations as needed.\n    if batch_id == 10:\n        break\n","metadata":{"id":"azrxUXP5Ln07","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Iterate over the train_dataloader with tqdm for progress tracking.\nfor batch_id, data in tqdm(enumerate(train_dataloader)):\n    # Extract input and target data from the batch.\n    x_data = data[0]\n    y_data = data[1]\n\n    # Print additional information or perform computations as needed.\n    if batch_id == 10:\n        break\n","metadata":{"id":"5J1XVCUCLn07","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that in this case just a simple generator is comparable or slightly faster than a proper data loader.","metadata":{"id":"Xcdx5-YbLn08"}},{"cell_type":"code","source":"def one_hot(args, num_classes=2):\n    # Generate one-hot encoding for each element in 'args'.\n    out = [[1 if idx == elem else 0 for idx in range(num_classes)] for elem in args]\n    return out\n\ndef argmax(args):\n    # Find the index of the maximum value in each array in 'args'.\n    out = [ivy.argmax(elem) for elem in args]\n    return out\n\n# Test the one_hot and argmax functions.\nprint(one_hot([0, 0, 1, 0]))\nprint(argmax(ivy.array([[0.49967843, 0.50032151],\n                       [0.49986687, 0.50013322],\n                       [0.49912587, 0.50087422],\n                       [0.50080854, 0.4991914 ],\n                       [0.50049627, 0.4995037 ],\n                       [0.4998956 , 0.50010443],\n                       [0.50008798, 0.49991205],\n                       [0.50053447, 0.49946556]])))","metadata":{"id":"SReEp9rmLn08","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the backend to \"torch\" for Ivy library.\nivy.set_backend(\"torch\")\n\n# Define various parameters and variables.\nnum_embeddings = tokenizer.vocab_size\nembedding_dim = 10\npad_token_id = tokenizer.pad_token_id\ninput_channels = embedding_dim\nnum_classes = 2\noutput_channels = 10\nnum_layers = 1\nmax_length = 128\nmax_length = max_length - 3\ntokenizer.model_max_length = max_length\neps = 1e-05\nbatch_size = 128\n\n# Generate testing input and labels.\ntesting_input = df.sample(batch_size)[\"title\"]\ntesting_labels = df.sample(batch_size)[\"label\"]\n\n# Calculate the linear input and output channels.\nlinear_input_channels = (tokenizer.model_max_length + 3) * batch_size * output_channels  # 3 comes from the hidden states of the LSTM\nlinear_output_channels = num_classes * batch_size\n\n# Define the normalized shape.\nnormalized_shape = (num_classes)\n\n# Define a custom module for post-processing LSTM output.\nclass LSTM_postproc(Module):\n    def __init__(self):\n        super(LSTM_postproc, self).__init__()\n\n    def _forward(self, args):\n        lstm_output, lstm_state = args\n        lstm_state_latest, lstm_state_hidden = lstm_state\n        lstm_state_latest = ivy.array(lstm_state_latest)\n        lstm_state_hidden = ivy.array([state for state in lstm_state_hidden][0])\n\n        lstm_state = ivy.concat((lstm_state_latest, lstm_state_hidden), axis=0).reshape((batch_size, 3, -1))\n        out = ivy.concat([lstm_output, lstm_state], axis=1)\n        out = out.flatten()\n        return out\n\n# Define a custom module for tokenizing input.\nclass Tokenizer(Module):\n    def __init__(self, tokenizer):\n        super(Tokenizer, self).__init__()\n        self.tokenizer = tokenizer\n\n    def _forward(self, args):\n        args = list(args)\n        return self.tokenizer(args, add_special_tokens=True, max_length=max_length, padding=\"max_length\", truncation=True)[\"input_ids\"]\n\n# Define a custom module for reshaping output.\nclass Reshaper(Module):\n    def __init__(self):\n        super(Reshaper, self).__init__()\n\n    def _forward(self, args):\n        return args.reshape((batch_size, num_classes))\n\n# Define a custom module for performing argmax operation.\nclass Argmax(Module):\n    def __init__(self):\n        super(Argmax, self).__init__()\n\n    def _forward(self, args):\n        return ivy.argmax(args, axis=-1)\n\n# Define a custom module for embedding.\nclass ivy_Embed(Module):\n    def __init__(self, embedding):\n        super(ivy_Embed, self).__init__()\n        self.embedding = embedding\n\n    def _forward(self, args):\n        out = self.embedding(args).float()\n        return out\n\n# Initialize the embedding layer.\nembedding = Embedding(num_embeddings, embedding_dim, pad_token_id)\n\n# Define the neural network architecture using Sequential.\nivy_LSTM = Sequential(\n    ivy_Embed(embedding),\n    LSTM(input_channels, output_channels, num_layers=num_layers, return_sequence=True, return_state=True, device=None, v=None, dtype=None),\n    LSTM_postproc(),\n    Linear(linear_input_channels, linear_output_channels, with_bias=True),\n    Reshaper(),\n    Sigmoid(),\n    Softmax(),\n)\n","metadata":{"id":"4dQ2Recq9Xo8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_tokens_loader(dataset = df):\n    train_loader = ivy_train_loader(dataset=dataset, batch_size=batch_size)\n    ivy_tokenizer = Tokenizer(tokenizer)\n    loaded_data = []\n    data_dict = dict()\n    for batch_id, data in tqdm(enumerate(train_loader)):\n\n        x_data = ivy.array(ivy_tokenizer(data[0]), device=device).long()\n        y_data = ivy.array(one_hot(list(data[1])), device=device)\n\n        loaded_data.append([x_data, y_data])\n        data_dict[f\"batch{batch_id}\"] = {\"x_data\": x_data, \"y_data\": y_data}\n        \n    loaded_data = ivy.array(loaded_data, device=device)\n    return loaded_data, data_dict\n\nloaded_tokens, tokens_dict = train_tokens_loader(df_full)\n\nwith open(\"tokenized_dataset.json\", \"w\") as outfile: \n    json.dump(tokens_dict, outfile)\n!cp \"tokenized_dataset.json\" \"/kaggle/working/demos/Contributor_demos/Sarcasm Detection/\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_ivy(model):\n    logs = []\n    learning_rate = 3e-4\n    opt = SGD(lr=learning_rate, inplace=True, stop_gradients=True, trace_on_next_step=False)\n    loss_fn = CrossEntropyLoss(axis=-1, epsilon=1e-07, reduction='mean')\n    epochs = 2\n    grads = ivy.zeros_like(model.v)\n    classifier = model\n    ivy_tokenizer = Tokenizer(tokenizer)\n\n    def loss(params):\n        v, model, x, y = params\n        predictions = model(x, v=v).flatten().to(device)\n        return loss_fn(y, predictions)\n\n    def one_hot(args, num_classes=2):\n        # Convert labels to one-hot encoding.\n        out = ivy.array([[1 if idx == elem else 0 for idx in range(num_classes)] for elem in args], device=device).flatten()\n        return out.long()\n    \n    for epoch in range(epochs):\n        # Iterate over the training loader for each epoch.\n        for batch_id, data in tqdm(enumerate(loaded_tokens):\n\n            x_data = ivy.array(ivy_tokenizer(data[0]), device=device).long()\n            y_data = one_hot(list(data[1]))\n\n            # Compute loss and gradients\n            loss_val, grads = ivy.execute_with_gradients(loss, (model.v, model, x_data, y_data))\n\n            # Update parameters using SGD optimizer\n            model.v = opt.step(model.v, grads)\n\n            if batch_id % 100 == 0:\n                # Log loss values periodically\n                logs.append([[epoch, batch_id, loss_val]])\n\n      # opt.clear_grad()\n    gc.collect()\n\n    return logs, model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n","metadata":{"id":"1NqBJx51TYSd"}},{"cell_type":"code","source":"# Train the Ivy LSTM model using the custom training function.\nlogs, ivy_LSTM = train_ivy(ivy_LSTM)\n","metadata":{"id":"7-5EmgSSLn0-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the trained Ivy LSTM model.\nivy_LSTM.save(\"Ivy_Sarcasm_Detection_Demo\")\nivy_LSTM.save_weights(\"/kaggle/working/demos/Contributor_demos/Sarcasm Detection/Ivy_Sarcasm_Detection_Demo_weights.hdf5\")\n\n# Copy the saved model to the specified directory.\n!cp \"Ivy_Sarcasm_Detection_Demo\" \"/kaggle/working/demos/Contributor_demos/Sarcasm Detection/\"\n","metadata":{"id":"K4gP_H5OLn0_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('logs.csv','w',encoding = 'utf-8') as f:\n    f.write(\"epoch, batch_id, loss_val\")\n    for entry in logs:\n        f.write(f\"{entry[0][0]}, {entry[0][1]}, {entry[0][2]}\")\n        \n!cp logs.csv \"/kaggle/working/demos/Contributor_demos/Sarcasm Detection\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the Ivy LSTM model to evaluation mode.\nivy_LSTM.eval()\n\n# Disable training mode for the Ivy LSTM model.\nivy_LSTM.train(False)\n","metadata":{"id":"2FFbG1JULn0_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_ivy(model):\n    logs = []\n    learning_rate = 3e-5\n    opt = SGD(lr=learning_rate, inplace=True, stop_gradients=True, trace_on_next_step=False)\n    loss_fn = CrossEntropyLoss(axis=-1, epsilon=1e-07, reduction='mean')\n    epochs = 2\n    grads = ivy.zeros_like(model.v)\n    classifier = model\n    train_loader = ivy_train_loader(dataset=df_eval, batch_size=batch_size)\n    ivy_tokenizer = Tokenizer(tokenizer)\n\n    def loss(params):\n        v, model, x, y = params\n        predictions = model(x, v=v).flatten()\n        return loss_fn(y, predictions)\n\n    def one_hot(args, num_classes=2):\n        # Convert labels to one-hot encoding.\n        out = ivy.array([[1 if idx == elem else 0 for idx in range(num_classes)] for elem in args]).flatten()\n        return out.long()\n    \n    for batch_id, data in tqdm(enumerate(loaded_data)):\n\n        x_data = ivy.array(ivy_tokenizer(data[0])).long()\n        y_data = one_hot(list(data[1])).long()\n\n        # Compute predictions\n        predictions = classifier(x_data).float()\n        predictions = ivy.argmax(predictions, axis=-1).flatten().float()\n        predictions = one_hot(list(predictions)).reshape(y_data.shape)\n\n        # Compute accuracy\n        acc = ivy.matmul(predictions.float(), y_data.float()).float() / batch_size\n\n        # Compute loss\n        loss_vals = loss((model.v, model, x_data, y_data))\n\n        # Append loss and accuracy to logs\n        logs.append([[loss_vals, acc]])\n\n    # Clear gradients and release memory\n    # opt.clear_grad()\n    gc.collect()\n\n    # Calculate mean loss and accuracy\n    return ivy.mean(logs, axis=0)\n","metadata":{"id":"ePSJNSfvLn0_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the Ivy LSTM model using the custom evaluation function.\nlogs_eval = eval_ivy(ivy_LSTM)","metadata":{"id":"xTk4aOCkLn0_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the evaluation results (mean loss and accuracy).\nprint(logs_eval)","metadata":{"id":"QOmaAFL4Ln1A","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the number of samples in the evaluation dataset.\nprint(len(df_eval))","metadata":{"id":"GF2kHSbyLn1A","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the train-test split ratio.\ntrain_test_ratio = 0.95\n\n# Define the fraction of the dataset to use.\nfrac_dataset = 1\n\n# Calculate the size of the full dataset.\ndf_size = len(df_full)\n\n# Calculate the index to split the dataset for training and evaluation.\nsplit = int(df_size * train_test_ratio * frac_dataset)\n\n# Define the cutoff index for the dataset fraction.\ncutoff = int(df_size * frac_dataset)\n\n# Extract the training dataset.\ndf = df_full.iloc[:split,:]\n\n# Extract the evaluation dataset.\ndf_eval = df_full.iloc[split:cutoff,:]\n","metadata":{"id":"ZqOd0SD7Ln1A","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the Ivy LSTM model using the custom training function.\nlogs, ivy_LSTM = train_ivy(ivy_LSTM)","metadata":{"id":"r1FZJkc6Ln1A","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the trained Ivy LSTM model.\nivy_LSTM.save(\"Ivy_Sarcasm_Detection_Demo\")\nivy_LSTM.save_weights(\"/kaggle/working/demos/Contributor_demos/Sarcasm Detection/Ivy_Sarcasm_Detection_Demo_weights.hdf5\")\n# Copy the saved model to the specified directory.\n!cp \"Ivy_Sarcasm_Detection_Demo\" /kaggle/working/demos/Contributor_demos/Sarcasm Detection\n","metadata":{"id":"h-2vjH8NLn1A","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('logs_full.csv','w',encoding = 'utf-8') as f:\n    f.write(\"epoch, batch_id, loss_val\")\n    for entry in logs:\n        f.write(f\"{entry[0][0]}, {entry[0][1]}, {entry[0][2]}\")\n        \n!cp logs.csv \"/kaggle/working/demos/Contributor_demos/Sarcasm Detection\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}