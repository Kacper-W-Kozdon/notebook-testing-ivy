{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":36545,"sourceType":"datasetVersion","datasetId":1309}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kacperkodo/sarcasm-detection-using-the-ivy-library?scriptVersionId=171030039\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# DEPENDANCIES AND SETUP","metadata":{"id":"s2B-C0ETR8j-"}},{"cell_type":"markdown","source":"Installing kaggle and uploading the API key necessary to use it.","metadata":{"id":"lVY3Z4myS1O4"}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-04-08T11:42:30.256077Z","iopub.execute_input":"2024-04-08T11:42:30.25678Z","iopub.status.idle":"2024-04-08T11:42:31.91639Z","shell.execute_reply.started":"2024-04-08T11:42:30.25675Z","shell.execute_reply":"2024-04-08T11:42:31.915489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q kaggle\n# from google.colab import files\n# from google.colab import userdata\nimport os\n# files.upload(); #Upload kaggle.json - you can get from the kaggle account settings, from the API section.","metadata":{"id":"7R4luV8tSDFn","outputId":"458b86d8-f64a-4581-aa23-5518e62f0623","execution":{"iopub.status.busy":"2024-04-08T11:42:31.918129Z","iopub.execute_input":"2024-04-08T11:42:31.918504Z","iopub.status.idle":"2024-04-08T11:42:46.555832Z","shell.execute_reply.started":"2024-04-08T11:42:31.918478Z","shell.execute_reply":"2024-04-08T11:42:46.554604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Installing packages necessary to use torch's transformers.","metadata":{"id":"GRyxyRkNqONt"}},{"cell_type":"code","source":"!pip install tqdm boto3 requests regex sentencepiece sacremoses botocore>=1.34.79","metadata":{"id":"yhD653HGqOj2","outputId":"4408500f-9c76-401e-adb7-05b71e5fa2f4","execution":{"iopub.status.busy":"2024-04-08T11:42:46.557483Z","iopub.execute_input":"2024-04-08T11:42:46.557862Z","iopub.status.idle":"2024-04-08T11:43:05.98773Z","shell.execute_reply.started":"2024-04-08T11:42:46.557823Z","shell.execute_reply":"2024-04-08T11:43:05.986802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To use the API, credentials need to be copied into the kaggle folder. If everything works, the output will show the list of available datasets.","metadata":{"id":"aCN2c1DGTbVM"}},{"cell_type":"code","source":"# !mkdir ~/.kaggle\n# !cp kaggle.json ~/.kaggle/\n# !chmod 600 ~/.kaggle/kaggle.json\n# !kaggle datasets list","metadata":{"id":"JfGZETFqTOla","outputId":"125442d7-1e01-4dd5-8136-43e41fb748d6","execution":{"iopub.status.busy":"2024-04-08T11:43:05.990859Z","iopub.execute_input":"2024-04-08T11:43:05.991698Z","iopub.status.idle":"2024-04-08T11:43:05.995963Z","shell.execute_reply.started":"2024-04-08T11:43:05.991661Z","shell.execute_reply":"2024-04-08T11:43:05.994928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"P1aQHs-9Tkt2"}},{"cell_type":"markdown","source":"Preparing the ivy library.","metadata":{"id":"_Sf8EImZT6kZ"}},{"cell_type":"code","source":"#Insert the correct user when cloning the repos. Make sure that they are up-to-date.\n\n!git clone \"https://github.com/Kacper-W-Kozdon/demos.git\"\n# !git clone \"https://github.com/Kacper-W-Kozdon/ivy.git\"\n!pip install -U -q paddlepaddle-gpu ivy accelerate>=0.21.0 mlflow datasets>=2.14.5 nlp 2>/dev/null # If ran in a notebook with only cpu enabled, edit \"paddlepaddle-gpu\" to \"paddlepaddle\"","metadata":{"id":"7DMn3EoEUBGQ","outputId":"357f988a-3c36-4b74-a10d-ff233047b17c","execution":{"iopub.status.busy":"2024-04-08T11:48:27.597129Z","iopub.execute_input":"2024-04-08T11:48:27.59804Z","iopub.status.idle":"2024-04-08T11:57:26.74944Z","shell.execute_reply.started":"2024-04-08T11:48:27.597992Z","shell.execute_reply":"2024-04-08T11:57:26.748138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next: import the ivy library and get the dataset.","metadata":{"id":"y1sA3gFuWjDE"}},{"cell_type":"code","source":"import ivy","metadata":{"id":"_NUgteS_Dluc","execution":{"iopub.status.busy":"2024-04-08T11:44:18.631353Z","iopub.execute_input":"2024-04-08T11:44:18.63169Z","iopub.status.idle":"2024-04-08T11:44:19.244766Z","shell.execute_reply.started":"2024-04-08T11:44:18.631659Z","shell.execute_reply":"2024-04-08T11:44:19.243819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !kaggle datasets download -d danofer/sarcasm\n# !cp -f sarcasm.zip '/kaggle/working/demos/Contributor_demos/Sarcasm Detection/'\n# !unzip '/kaggle/working/demos/Contributor_demos/Sarcasm Detection/sarcasm.zip' -d '/kaggle/working/demos/Contributor_demos/Sarcasm Detection/'","metadata":{"id":"7CA3MzIzUayI","outputId":"35d41ed3-9911-4b88-be53-55c8c54abf20","execution":{"iopub.status.busy":"2024-04-08T11:44:19.246007Z","iopub.execute_input":"2024-04-08T11:44:19.246678Z","iopub.status.idle":"2024-04-08T11:44:19.251107Z","shell.execute_reply.started":"2024-04-08T11:44:19.246643Z","shell.execute_reply":"2024-04-08T11:44:19.250142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import the libraries suggested in the model which is to be transpiled.","metadata":{"id":"UKN-VX8QXDEG"}},{"cell_type":"code","source":"import paddle","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd  # For data manipulation and analysis\nimport gc  # For garbage collection to manage memory\nimport re  # For regular expressions\nimport numpy as np  # For numerical operations and arrays\nimport tensorflow as tf\nimport torch  # PyTorch library for deep learning\n\nimport ivy.functional.frontends.paddle as paddle_frontend\n\n# Libraries to accompany torch's transformers\nimport tqdm\nimport boto3\nimport requests\nimport regex\nimport sentencepiece\nimport sacremoses\n\nimport warnings  # For handling warnings\nwarnings.filterwarnings(\"ignore\")  # Ignore warning messages\n\nfrom transformers import AutoModel, AutoTokenizer  # Transformers library for natural language processing\n# from transformers import TextDataset, LineByLineTextDataset, DataCollatorForLanguageModeling, \\\n# pipeline, Trainer, TrainingArguments, DataCollatorWithPadding  # Transformers components for text processing\nfrom transformers import TextDataset, LineByLineTextDataset, DataCollatorForLanguageModeling, \\\npipeline, TrainingArguments, DataCollatorWithPadding\nfrom transformers import AutoModelForSequenceClassification  # Transformer model for sequence classification\n\nimport accelerate\n\n# from nlp import Dataset  # Import custom 'Dataset' class for natural language processing tasks\nfrom imblearn.over_sampling import RandomOverSampler  # For oversampling to handle class imbalance\n# import datasets  # Import datasets library\n# from datasets import Dataset, Image, ClassLabel  # Import custom 'Dataset', 'ClassLabel', and 'Image' classes\nfrom transformers import pipeline  # Transformers library for pipelines\nfrom bs4 import BeautifulSoup  # For parsing HTML content\n\nimport matplotlib.pyplot as plt  # For data visualization\nimport itertools  # For working with iterators\nfrom sklearn.metrics import (  # Import various metrics from scikit-learn\n    accuracy_score,  # For calculating accuracy\n    roc_auc_score,  # For ROC AUC score\n    confusion_matrix,  # For confusion matrix\n    classification_report,  # For classification report\n    f1_score  # For F1 score\n)\n\n# from datasets import load_metric  # Import load_metric function to load evaluation metrics\n\nfrom tqdm import tqdm  # For displaying progress bars\n\ntqdm.pandas()  # Enable progress bars for pandas operations","metadata":{"id":"19rgBXHJXHFu","execution":{"iopub.status.busy":"2024-04-08T11:44:19.25255Z","iopub.execute_input":"2024-04-08T11:44:19.252906Z","iopub.status.idle":"2024-04-08T11:44:52.732775Z","shell.execute_reply.started":"2024-04-08T11:44:19.252874Z","shell.execute_reply":"2024-04-08T11:44:52.731943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"gpu:0\" if paddle.device.cuda.device_count() else \"cpu\" # Either \"gpu\" or \"gpu:0\".\nivy.set_default_device(device)\nivy.set_soft_device_mode(True)\n","metadata":{"id":"bXr9tGFLGRPI","execution":{"iopub.status.busy":"2024-04-08T11:44:52.73392Z","iopub.execute_input":"2024-04-08T11:44:52.734579Z","iopub.status.idle":"2024-04-08T11:44:52.740185Z","shell.execute_reply.started":"2024-04-08T11:44:52.73454Z","shell.execute_reply":"2024-04-08T11:44:52.739337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(ivy.default_device())\nprint(ivy.num_gpus())\n","metadata":{"id":"ijs6fSKL9QZ4","outputId":"23f84802-981c-4869-ea4c-c9a94489e2ff","execution":{"iopub.status.busy":"2024-04-08T16:13:43.18775Z","iopub.execute_input":"2024-04-08T16:13:43.188062Z","iopub.status.idle":"2024-04-08T16:13:43.576815Z","shell.execute_reply.started":"2024-04-08T16:13:43.188037Z","shell.execute_reply":"2024-04-08T16:13:43.575492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Set the seeds.","metadata":{"id":"JU7qbxYdsVlK"}},{"cell_type":"code","source":"tf.keras.utils.set_random_seed(0)\ntorch.manual_seed(0)\npaddle.seed(0)","metadata":{"id":"HxD1xridsU_l","outputId":"1b43d30a-d4bb-401b-f540-988ccbc9b24a","execution":{"iopub.status.busy":"2024-04-08T11:44:52.793055Z","iopub.execute_input":"2024-04-08T11:44:52.793304Z","iopub.status.idle":"2024-04-08T11:44:52.804919Z","shell.execute_reply.started":"2024-04-08T11:44:52.793282Z","shell.execute_reply":"2024-04-08T11:44:52.803975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get the API key for ivy transpiler from your account and upload it to the project. Move it to the correct directory.","metadata":{"id":"zwU4oNrkXyxT"}},{"cell_type":"code","source":"pwd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# files.upload(); #Upload key.pem - you can get from the kaggle account settings, from the API section.\n\nfrom kaggle_secrets import UserSecretsClient\nsecret_label = \"key.pem\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\n\nwith open('/kaggle/working/key.pem','w+') as ivy_api_key:\n    ivy_api_key.write(secret_value)\n\n","metadata":{"id":"LME9LyKaXyVF","outputId":"8568ff9c-19d6-4947-fba8-77670684d6c6","execution":{"iopub.status.busy":"2024-04-08T10:45:51.208661Z","iopub.status.idle":"2024-04-08T10:45:51.209107Z","shell.execute_reply.started":"2024-04-08T10:45:51.208876Z","shell.execute_reply":"2024-04-08T10:45:51.208894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir ~/.ivy #It might be necessary to change \".ivy\" to \"ivy\".\n!cp key.pem /kaggle/working/.ivy","metadata":{"id":"NbuJUunHYIwg","execution":{"iopub.status.busy":"2024-04-08T10:45:51.210656Z","iopub.status.idle":"2024-04-08T10:45:51.211135Z","shell.execute_reply.started":"2024-04-08T10:45:51.210883Z","shell.execute_reply":"2024-04-08T10:45:51.210903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp key.pem /kaggle/working/.ivy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First we're loading the tokenizer and the model from torch. All of the basic set-up instructions can be found here: https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/huggingface_pytorch-transformers.ipynb#scrollTo=72d8f2de","metadata":{"id":"DWCnfosUshGK"}},{"cell_type":"code","source":"tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\nmodel = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\n\nsequence_classifier = torch.hub.load('huggingface/pytorch-transformers', 'modelForSequenceClassification', 'bert-base-cased')","metadata":{"id":"2rZY3rhisgXZ","outputId":"6737a6a8-76a2-4881-dcaa-1e2fa12372b4","execution":{"iopub.status.busy":"2024-04-08T10:45:51.21249Z","iopub.status.idle":"2024-04-08T10:45:51.212977Z","shell.execute_reply.started":"2024-04-08T10:45:51.212718Z","shell.execute_reply":"2024-04-08T10:45:51.212738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ivy.stateful.module import Module\nfrom ivy.stateful.sequential import Sequential\nfrom ivy.stateful.layers import *\nfrom ivy.stateful.losses import *\nfrom ivy.stateful.optimizers import *\nfrom ivy.stateful.activations import *\nfrom ivy.stateful.initializers import *\nfrom ivy.stateful.norms import *\n","metadata":{"id":"b0MdcERZYS_6","execution":{"iopub.status.busy":"2024-04-08T10:45:51.214634Z","iopub.status.idle":"2024-04-08T10:45:51.215096Z","shell.execute_reply.started":"2024-04-08T10:45:51.214857Z","shell.execute_reply":"2024-04-08T10:45:51.214876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATASET AND MODEL OVERVIEW","metadata":{"id":"bXFPiT6SgPob"}},{"cell_type":"code","source":"!echo -n API_KEY > .ivy/key.pem","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ivy.set_backend(\"tensorflow\")\n#tokenizer_tf = ivy.transpile(tokenizer, source=\"torch\", to=\"tensorflow\")\n#model_pd = ivy.to_ivy_module(model)\n#model_pd = model_pd.trace_graph()\n#model_pd = model_pd.set_backend(\"tensorflow\")\nmodel_tf = ivy.transpile(model, source=\"torch\", to=\"tensorflow\")\nsequence_classifier_tf = ivy.transpile(sequence_classifier, source=\"torch\", to=\"tensorflow\")","metadata":{"id":"W6alZ3W1VtOz","execution":{"iopub.status.busy":"2024-04-08T10:45:51.2161Z","iopub.status.idle":"2024-04-08T10:45:51.216671Z","shell.execute_reply.started":"2024-04-08T10:45:51.216397Z","shell.execute_reply":"2024-04-08T10:45:51.216418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = pd.read_csv(\"/content/demos/Contributor_demos/Sarcasm Detection/train-balanced-sarcasm.csv\")\ndf = pd.read_csv(\"/kaggle/input/sarcasm/train-balanced-sarcasm.csv\")\ndf = df.drop_duplicates()\ndf = df.rename(columns={'comment': 'title'})\ndf = df[['label', 'title']]\ndf = df[~df['label'].isnull()]\ndf = df[~df['title'].isnull()]\ndf.sample(5)","metadata":{"id":"5RdJOhqgYDph","outputId":"602cced0-716a-48cc-d578-76318c0f662c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_words(text: str) -> int:\n  return len(text.split())\n\ndef count_symbols(text: str) -> int:\n  return len(\"\".join(text.split()))\n\ndef symbol_to_word_ratio(text: str) -> float:\n  return count_symbols(text)/count_words(text)\n\ndef upper_lower_ratio(text: str) -> float:\n  text = \"\".join(text.split())\n  return sum(1 for c in text if c.isupper())/(max([sum(1 for c in text if c.islower()), 1]))\n\ndf['word_count'] = df[\"title\"].apply(count_words)\ndf['symbol_count'] = df[\"title\"].apply(count_symbols)\ndf[\"upper_lower_ratio\"] = df[\"title\"].apply(upper_lower_ratio)\ndf[\"symbol_to_word_ratio\"] = df[\"title\"].apply(symbol_to_word_ratio)\ndf.sample(5)","metadata":{"id":"BVA6U5Y0c7vg","outputId":"13606496-e191-4005-a04a-4e516875bdfa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A few plots to see some some characteristics of the data.","metadata":{"id":"YY9ru5DbJqxa"}},{"cell_type":"code","source":"df_no_sarc = df.where(df[\"label\"] == 0)\ndf_no_sarc = df_no_sarc.where(df_no_sarc[\"word_count\"] <= 51)\ndf_sarc = df.where(df[\"label\"] == 1)\ndf_sarc = df_sarc.where(df_sarc[\"word_count\"] <= 51)\ndf_no_sarc = df_no_sarc[np.isfinite(df_no_sarc[\"word_count\"])]\ndf_sarc = df_sarc[np.isfinite(df_sarc[\"word_count\"])]\nplt.style.use('_mpl-gallery-nogrid')\n\nhist_df_no_sarc, bin_edges_no = np.histogram(df_no_sarc[\"word_count\"].values, density=True)\nhist_df_sarc, bin_edges = np.histogram(df_sarc[\"word_count\"].values, density=True)\n# plot:\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nbin_mids_no = [(bin_edges_no[i+1] + bin_edges_no[i])/2 for i in range(len(bin_edges_no) - 1)]\nbin_mids = [(bin_edges[i+1] + bin_edges[i])/2 for i in range(len(bin_edges) - 1)]\nax1.bar(bin_mids_no, hist_df_no_sarc, width=bin_edges_no[1] - bin_edges_no[0])\nax2.bar(bin_mids, hist_df_sarc, width=bin_edges[1] - bin_edges[0])\nax1.set_title(\"Hist no sarcasm\")\nax1.set_ylabel(\"density\")\nax1.set_xlabel(\"word count\")\nax1.set_xticks(bin_edges_no)\nax1.grid(True)\nax2.set_title(\"Hist sarcasm\")\nax2.set_xlabel(\"word count\")\nax2.set_xticks(bin_edges)\nax2.grid(True)\nplt.show()","metadata":{"id":"_LEHSFedgIBq","outputId":"f6f7cb26-7383-4fa9-f454-b7f07adba063","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_no_sarc = df.where(df[\"label\"] == 0)\ndf_no_sarc = df_no_sarc.where(df_no_sarc[\"symbol_count\"] <= 201)\ndf_sarc = df.where(df[\"label\"] == 1)\ndf_sarc = df_sarc.where(df_sarc[\"symbol_count\"] <= 201)\ndf_no_sarc = df_no_sarc[np.isfinite(df_no_sarc[\"symbol_count\"])]\ndf_sarc = df_sarc[np.isfinite(df_sarc[\"symbol_count\"])]\nplt.style.use('_mpl-gallery-nogrid')\n\nhist_df_no_sarc, bin_edges_no = np.histogram(df_no_sarc[\"symbol_count\"].values, density=True)\nhist_df_sarc, bin_edges = np.histogram(df_sarc[\"symbol_count\"].values, density=True)\n# plot:\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nbin_mids_no = [(bin_edges_no[i+1] + bin_edges_no[i])/2 for i in range(len(bin_edges_no) - 1)]\nbin_mids = [(bin_edges[i+1] + bin_edges[i])/2 for i in range(len(bin_edges) - 1)]\nax1.bar(bin_mids_no, hist_df_no_sarc, width=bin_edges_no[1] - bin_edges_no[0])\nax2.bar(bin_mids, hist_df_sarc, width=bin_edges[1] - bin_edges[0])\nax1.set_title(\"Hist no sarcasm\")\nax1.set_ylabel(\"density\")\nax1.set_xlabel(\"symbol count\")\nax1.set_xticks(bin_edges_no)\nax1.grid(True)\nax2.set_title(\"Hist sarcasm\")\nax2.set_xlabel(\"symbol count\")\nax2.set_xticks(bin_edges)\nax2.grid(True)\nplt.show()","metadata":{"id":"RcYhYzfygLc9","outputId":"df9ed393-4d87-4431-b393-76729e77cb97","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_no_sarc = df.where(df[\"label\"] == 0)\ndf_no_sarc = df_no_sarc.where(df_no_sarc[\"upper_lower_ratio\"] <= 0.3)\ndf_sarc = df.where(df[\"label\"] == 1)\ndf_sarc = df_sarc.where(df_sarc[\"upper_lower_ratio\"] <= 0.3)\ndf_no_sarc = df_no_sarc[np.isfinite(df_no_sarc[\"upper_lower_ratio\"])]\ndf_sarc = df_sarc[np.isfinite(df_sarc[\"upper_lower_ratio\"])]\nplt.style.use('_mpl-gallery-nogrid')\n\nhist_df_no_sarc, bin_edges_no = np.histogram(df_no_sarc[\"upper_lower_ratio\"].values, density=True)\nhist_df_sarc, bin_edges = np.histogram(df_sarc[\"upper_lower_ratio\"].values, density=True)\n# plot:\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nbin_mids_no = [(bin_edges_no[i+1] + bin_edges_no[i])/2 for i in range(len(bin_edges_no) - 1)]\nbin_mids = [(bin_edges[i+1] + bin_edges[i])/2 for i in range(len(bin_edges) - 1)]\nax1.bar(bin_mids_no, hist_df_no_sarc, width=bin_edges_no[1] - bin_edges_no[0])\nax2.bar(bin_mids, hist_df_sarc, width=bin_edges[1] - bin_edges[0])\nax1.set_title(\"Hist no sarcasm\")\nax1.set_ylabel(\"density\")\nax1.set_xlabel(\"upper/lower ratio\")\nax1.set_xticks(bin_edges_no)\nax1.grid(True)\nax2.set_title(\"Hist sarcasm\")\nax2.set_xlabel(\"upper/lower ratio\")\nax2.set_xticks(bin_edges)\nax2.grid(True)\nplt.show()","metadata":{"id":"vvKbuhLaDaSP","outputId":"f7756960-1a3d-4ace-b6a0-8addf8b6e53c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_no_sarc = df.where(df[\"label\"] == 0)\ndf_no_sarc = df_no_sarc.where(df_no_sarc[\"symbol_to_word_ratio\"] <= 11)\ndf_sarc = df.where(df[\"label\"] == 1)\ndf_sarc = df_sarc.where(df_sarc[\"symbol_to_word_ratio\"] <= 11)\ndf_no_sarc = df_no_sarc[np.isfinite(df_no_sarc[\"symbol_to_word_ratio\"])]\ndf_sarc = df_sarc[np.isfinite(df_sarc[\"symbol_to_word_ratio\"])]\nplt.style.use('_mpl-gallery-nogrid')\n\nhist_df_no_sarc, bin_edges_no = np.histogram(df_no_sarc[\"symbol_to_word_ratio\"].values, density=True)\nhist_df_sarc, bin_edges = np.histogram(df_sarc[\"symbol_to_word_ratio\"].values, density=True)\n# plot:\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nbin_mids_no = [(bin_edges_no[i+1] + bin_edges_no[i])/2 for i in range(len(bin_edges_no) - 1)]\nbin_mids = [(bin_edges[i+1] + bin_edges[i])/2 for i in range(len(bin_edges) - 1)]\nax1.bar(bin_mids_no, hist_df_no_sarc, width=bin_edges_no[1] - bin_edges_no[0])\nax2.bar(bin_mids, hist_df_sarc, width=bin_edges[1] - bin_edges[0])\nax1.set_title(\"Hist no sarcasm\")\nax1.set_ylabel(\"density\")\nax1.set_xlabel(\"symbols/words ratio\")\nax1.set_xticks(bin_edges_no)\nax1.grid(True)\nax2.set_title(\"Hist sarcasm\")\nax2.set_xlabel(\"symbols/words ratio\")\nax2.set_xticks(bin_edges)\nax2.grid(True)\nplt.show()","metadata":{"id":"HkuIzb1JF1U1","outputId":"7ab4df1a-bd8e-4188-d65b-f7d7c5075c17","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking if the tokenizer, encoder/decoder and classifier work.","metadata":{"id":"86gm5mdio9Hf"}},{"cell_type":"code","source":"input = df[\"title\"][1]\nprint(f\"The raw input: \\n{input}\\n\")\ntoken = tokenizer(input, return_tensors=\"pt\", add_special_tokens=True)\nprint(f\"The token: \\n{token}\\n\")\nwith torch.no_grad():\n  encoded_token = model(**token)\nprint(f\"The encoded token: \\n{encoded_token}\\n\")","metadata":{"id":"8UvKrBTwpECS","outputId":"cb7f09c3-ef66-4e17-b85a-387812b36283","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking if the transpiled tokenizer, encoder/decoder and classifier work.","metadata":{"id":"kcogrroTrFTg"}},{"cell_type":"code","source":"input = df[\"title\"][1]\nprint(f\"The raw input: \\n{input}\\n\")\ntoken = tokenizer.encode(input, return_tensors=\"tf\", add_special_tokens=True).cpu()\nprint(f\"The token: \\n{token}\\n\")\n#input_ids, token_type_ids, attention_mask = token[\"input_ids\"], token[\"token_type_ids\"], token[\"attention_mask\"]\nencoded_token = model_tf(token)\nprint(f\"The encoded token: \\n{encoded_token}\\n\")","metadata":{"id":"mYy0M5oapJjD","outputId":"f2780ba3-6c34-4299-c63c-dbe80520f819","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL TRANSPILATION","metadata":{"id":"YbygAR7ygfyL"}},{"cell_type":"markdown","source":"A quick check whether transpiling to paddle works as intended.","metadata":{"id":"MP4JRoYbGbpC"}},{"cell_type":"code","source":"class Network(torch.nn.Module):\n\n    def __init__(self):\n     super().__init__()\n     self._linear = torch.nn.Linear(3, 3)\n\n    def forward(self, x):\n     return self._linear(x)\n\nx = torch.tensor([1., 2., 3.])\nnet = Network()\nnet(x)","metadata":{"id":"kEUO3sSCrRYf","outputId":"4fffa3b2-2f80-45e0-e19c-661a21d90e12","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ivy.set_backend(\"paddle\")\nnet_pd = ivy.transpile(net, source=\"torch\", to=\"paddle\")\nx_pd = paddle.to_tensor([1., 2., 3.]).cpu()\nnet_pd(x_pd)","metadata":{"id":"5y7xNkstJKas","outputId":"171dbb37-160e-4bf4-ccbc-7c45e88d102f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Setting up the classifier based on BERT's sequence classifier model.","metadata":{"id":"hB-1iHnoBjBZ"}},{"cell_type":"code","source":"class Classifier(torch.nn.Module):\n    def __init__(self, num_classes=2):\n        super(Classifier, self).__init__()\n        self.tokenizer = tokenizer\n        self.model = sequence_classifier\n        self.softmax = torch.nn.functional.softmax\n\n    def forward(self, x):\n        # print(f\"The input: {x}\")\n        x = self.tokenizer(x, return_tensors=\"pt\", add_special_tokens=True, padding=True, truncation=True)\n        x = self.model(**x)\n        x = self.softmax(x[\"logits\"], dim=1)\n        return x\n\n\nivy.set_backend(\"paddle\")\nclassifier = Classifier()\nclassifier_paddle = ivy.transpile(classifier, source=\"torch\", to=\"paddle\")","metadata":{"id":"m3gMHZMdovuU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dir(classifier))\n# print(list(classifier.named_parameters()))","metadata":{"id":"OF2CmoOOu-Ok","outputId":"fe5ee5f8-78e4-41b3-842c-f7e3b83c99d4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Testing the transpilation of the BERT part of the model.","metadata":{"id":"_ThlDy1gYLxG"}},{"cell_type":"code","source":"# ivy.set_backend(\"torch\")\n# sequence_classifier_paddle = ivy.transpile(sequence_classifier, source=\"torch\", to=\"paddle\")","metadata":{"id":"KseIygIVKoIG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ivy.set_backend(\"paddle\")\ntokens = tokenizer([\"This is it\", \"This is not it.\"], add_special_tokens=True, padding=True, truncation=True)\ninput_ids, token_type_ids, attention_mask = ivy.array(tokens[\"input_ids\"]), ivy.array(tokens[\"token_type_ids\"]), paddle.to_tensor(tokens[\"attention_mask\"])\ntokens = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask}\nkwargs = {\"input_ids\": input_ids.cpu()}\nsequence_classifier_paddle = ivy.transpile(sequence_classifier, kwargs=kwargs, source=\"torch\", to=\"paddle\")","metadata":{"id":"_t3BckUO9Dmj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ivy.set_backend(\"paddle\")\ntokens = tokenizer([\"This is it\", \"This is not it.\"], add_special_tokens=True, padding=True, truncation=True)\nprint(tokens)\ninput_ids, token_type_ids, attention_mask = tokens[\"input_ids\"], tokens[\"token_type_ids\"], tokens[\"attention_mask\"]\ninput_ids, token_type_ids, attention_mask = paddle.to_tensor(tokens[\"input_ids\"]), paddle.to_tensor(tokens[\"token_type_ids\"]), paddle.to_tensor(tokens[\"attention_mask\"])\n# input_ids, token_type_ids, attention_mask = input_ids.to_native(), token_type_ids.to_native(), attention_mask.to_native()\ntokens = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask}\nprint(tokens)\nprint(sequence_classifier_paddle(input_ids=tokens[\"input_ids\"]))\nsequence_classifier_paddle(**tokens)","metadata":{"id":"BoazwSETBzq2","outputId":"7f9089a6-83f5-43c4-867b-ccee4d34dc5c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Testing the model put together (tokenizer, transpiled BERT, softmax)","metadata":{"id":"v5j9Cw4dYQ-4"}},{"cell_type":"code","source":"# ivy.set_backend(\"paddle\")\n# tokens = tokenizer([\"This is it\", \"This is not it.\"], add_special_tokens=True, padding=True, truncation=True)\n# input_ids, token_type_ids, attention_mask = ivy.array(tokens[\"input_ids\"]), ivy.array(tokens[\"token_type_ids\"]), ivy.array(tokens[\"attention_mask\"])\n# tokens = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask}\n# kwargs = {\"input_ids\": input_ids.cpu()}\n\n# sequence_classifier_paddle = ivy.transpile(sequence_classifier, kwargs=kwargs, source=\"torch\", to=\"paddle\")\n\n","metadata":{"id":"ZsVUuAn0yr1x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Classifier_paddle(paddle.nn.Layer):\n\n    def __init__(self, num_classes=2):\n        super(Classifier_paddle, self).__init__()\n        self.tokenizer = tokenizer\n        self.model = sequence_classifier_paddle\n        self.softmax = paddle.nn.functional.softmax\n\n    def forward(self, x):\n        # print(f\"The input: {x}\")\n        x = self.tokenizer(x, add_special_tokens=True, padding=True, truncation=True)\n        input_ids, token_type_ids, attention_mask = paddle.to_tensor(x[\"input_ids\"]), paddle.to_tensor(x[\"token_type_ids\"]), paddle.to_tensor(x[\"attention_mask\"])\n        x = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask}\n        x = self.model(**x)\n        x = self.softmax(x[\"logits\"], axis=1)\n        return x\n\nclassifier_paddle = Classifier_paddle()","metadata":{"id":"AGnSSaOhXgSs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(list(dir(classifier_paddle)))\n","metadata":{"id":"hnjH_ORPuRph","outputId":"ae438ca6-62dd-410a-dcfd-f890e1509f21"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dir(classifier_paddle.model))\n# print(classifier_paddle.model.__getattr__('bert/embeddings/LayerNorm/weight'))\nprint(list(classifier_paddle.model.named_parameters())[-2:])\n","metadata":{"id":"wGOL2QU2wVlC","outputId":"f2f6ea91-3381-417a-ffec-df88e9d00309"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are freezing the layers responsible for embedding, pooling, etc.","metadata":{"id":"LWYyqvnO0Z9d"}},{"cell_type":"code","source":"for layer in list(classifier_paddle.model.parameters())[-2:]:\n  layer.trainable = True\n\nfor layer in list(classifier_paddle.model.named_parameters())[-2:]:\n  print(layer)\nfor layer in list(classifier_paddle.model.parameters())[-2:]:\n  print(layer)\n\nprint(sum([1 if layer.trainable else 0 for layer in list(classifier_paddle.parameters())[:]]))\n\nfor layer in list(classifier_paddle.model.parameters())[:-2]:\n  layer.trainable = False","metadata":{"id":"5yIWw8SxzeKb","outputId":"fd442742-e6ae-4512-c25b-0c4125d61cc1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"id":"pZkigG9-wKBl","outputId":"4895637b-f6f5-4a60-db97-85382ee825eb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input = df[\"title\"][1]\ninput2 = df[\"title\"][5]\ninput3 = df[\"title\"][2]\n\nprint(input)\nprint(classifier([input]))\nprint(classifier([input, input2, input3]))","metadata":{"id":"cEhEgFY-xGCX","outputId":"c09f492c-fd8c-4eb8-84ad-7ebabd468614","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ivy.set_backend(\"paddle\")\nprint(classifier_paddle([input]))\nprint(classifier_paddle([input2]))\nprint(classifier_paddle([input, input2, input3]))\nprint(f\"Layers: {len(classifier_paddle.parameters())}\")","metadata":{"id":"cH6qLA0luFVr","outputId":"14886d51-c506-4fcd-c68b-67f76a60446b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'd like to make the cell below run as is but there still are some issues with how the arguments are passed to finish the transpilation in the lazy mode.","metadata":{"id":"a9sov2R-Yiha"}},{"cell_type":"code","source":"# tokens = tokenizer([\"Just one sentence\"], add_special_tokens=True, padding=True, truncation=True)\n# print(tokens)\n# input_ids, token_type_ids, attention_mask = ivy.array(tokens[\"input_ids\"]), ivy.array(tokens[\"token_type_ids\"]), ivy.array(tokens[\"attention_mask\"])\n# tokens = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask}\n# print(type(input_ids), input_ids, token_type_ids, attention_mask)\n# print(sequence_classifier_paddle(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask))\n# sequence_classifier_paddle(**tokens)","metadata":{"id":"BCrQ0IEJ_d92","outputId":"8bab2da3-2230-4141-ce92-5ba99409bc16"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(sequence_classifier_paddle))","metadata":{"id":"-htB7BNHY8WR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens = tokenizer([\"Just one sentence\"], add_special_tokens=True, padding=True, truncation=True)\nprint(tokens)\ninput_ids, token_type_ids, attention_mask = paddle.to_tensor(tokens[\"input_ids\"]), paddle.to_tensor(tokens[\"token_type_ids\"]), paddle.to_tensor(tokens[\"attention_mask\"])\ntokens = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask}\nprint(type(input_ids), input_ids, token_type_ids, attention_mask)\nprint(sequence_classifier_paddle(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask))\nsequence_classifier_paddle(**tokens)","metadata":{"id":"FsKjUc0RSur8","outputId":"6bcdb39c-d4ed-4a57-f258-5650e458adbf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Setting up the training and training the model.","metadata":{"id":"bGiRydXsFkdR"}},{"cell_type":"code","source":"def one_hot(input):\n\n  input = paddle.to_tensor(input)\n  return paddle.nn.functional.one_hot(input, num_classes=2)\n\nsample = df.sample(5)\nsample['label'] = sample['label'].apply(one_hot, \"columns\")\n\ntype(sample[\"label\"])\nfor label in sample[\"label\"]:\n  print(label)\n","metadata":{"id":"p7kKe5llRz5q","outputId":"d4eea1e1-0591-4d41-f82e-c28987930390"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['word_count',\t'symbol_count',\t'upper_lower_ratio',\t'symbol_to_word_ratio'], axis=1, inplace=True)\ngc.collect()","metadata":{"id":"v4EHZqustDht","outputId":"1b174382-9680-409a-e5a5-80100f03be2d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import paddle.distributed as dist\ndef one_hot(input):\n  input = paddle.to_tensor(input)\n  return paddle.nn.functional.one_hot(input, num_classes=2)\n\n\n\n# if type(df['label'][1]) is np.int64:\n#   df['label_one_hot'] = df['label'].apply(one_hot, \"columns\")\n\n# train_dataset = df[['title', 'label_one_hot']]\n# test_dataset = df[['title', 'label_one_hot']]\ntrain_dataset = df[['title', 'label']]\ntest_dataset = df[['title', 'label']]","metadata":{"id":"V9QJNQepK4U1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_dataset.info())\ntrain_dataset_sample = train_dataset.sample(3200)\nprint(train_dataset_sample.info())","metadata":{"id":"_9ZhIcGX95tT","outputId":"032d09fc-346a-4a09-a9fe-d16bcc5feb6c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_train = train_dataset.sample(5)\n# for entry, label in zip(sample_train[\"title\"], sample_train[\"label_one_hot\"]):\n#   print(entry, label)","metadata":{"id":"hIZPTinjiDtx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Switching from pandas.dataframe to paddle.io.Dataset for the convenience of use and compatibility.","metadata":{"id":"cgVOZIuT4Tpl"}},{"cell_type":"code","source":"gc.collect()","metadata":{"id":"nk3vK1u3FlZc","outputId":"75d850b4-d36e-4f97-984c-92ef21501d3a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom paddle.io import Dataset\n\nbatch_size = 8\n\n# define a random dataset\nclass pd_Dataset(Dataset):\n    def __init__(self, df):\n      super().__init__()\n      self.num_samples = df['title'].size\n      self.data = [[entry[0], entry[1]] for entry in zip(df[\"title\"], df[\"label\"])]\n\n    def __getitem__(self, idx):\n        title = self.data[idx][0]\n        label = self.data[idx][1]\n        return title, label\n\n    def __len__(self):\n        return self.num_samples\n\ntrain_dataset_pd = pd_Dataset(train_dataset)\n\ntrain_loader = paddle.io.DataLoader(train_dataset_pd, batch_size=batch_size, shuffle=True)\n","metadata":{"id":"4QJ40IAD1AqI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch_id, data in enumerate(train_loader()):\n  if batch_id > 1:\n    break\n  print(data[0])\n  print(data[1])","metadata":{"id":"rIbN7rd9tuXK","outputId":"008eb6e8-17bc-4e38-90bf-aafab24f86fc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef train(model):\n  logs = []\n  learning_rate = 3e-5\n  parameters = model.parameters()\n  print(f\"Trainable parameters: {sum([1 if layer.trainable else 0 for layer in list(classifier_paddle.parameters())[:]])}\")\n  opt = paddle.optimizer.SGD(learning_rate=learning_rate, parameters=model.parameters())\n  loss_fn = paddle.nn.CrossEntropyLoss(use_softmax=False)\n  metric = paddle.metric.Accuracy()\n  epochs = 2\n  loss = 0\n  acc = 0\n  classifier = paddle.DataParallel(model)\n  scaler = paddle.amp.GradScaler(init_loss_scaling=1024)\n  model, opt = paddle.amp.decorate(models=model, optimizers=opt, level='O2', master_weight=None, save_dtype=None)\n\n  for epoch in range(epochs):\n\n    for batch_id, data in tqdm(enumerate(train_loader()), postfix={\"epoch\": epoch, \"loss\": loss, \"acc\": acc}):\n\n      x_data = data[0]\n      y_data = data[1]\n\n      with paddle.amp.auto_cast(enable=True, custom_white_list=None, custom_black_list=None, level='O2'):\n        predicts = classifier(x_data) # The transpiled model seems to have problems with inputs, so instead of feeding it a container, we map onto one.\n        loss = loss_fn(predicts, y_data)\n\n      correct = metric.compute(predicts, y_data)\n      metric.update(correct)\n      acc = metric.accumulate()\n      # acc = paddle.metric.accuracy(predicts, y_data) # This needs to be corrected.\n      scaled = scaler.scale(loss)\n      scaled.backward()\n\n      # update parameters\n      scaler.minimize(opt, scaled)\n\n      if batch_id % 100 == 0:\n          # print(\"\\nepoch: {}, batch_id: {}, loss is: {}, acc is: {}\".format(epoch, batch_id, loss.numpy(), acc))\n          logs.append([epoch, batch_id, loss.numpy(), acc])\n\n      opt.clear_grad()\n    gc.collect()\n\n\n  obj = {'model': model.state_dict(), 'opt': opt.state_dict(), 'epoch': epochs}\n  path = '/content/demos/Contributor_demos/Sarcasm Detection/model.pdparams'\n  paddle.save(obj, path)\n\n  return logs, model\n","metadata":{"id":"LLZwipjSDGAf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for batch_id, data in enumerate(train_loader()):\n#   print(data[0], data[1])\n#   print(classifier_paddle(data[0]))\n#   if batch_id >= 1:\n#     break","metadata":{"id":"4k21SfgTrEKL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classifier_paddle)\nprint(sequence_classifier_paddle)\nprint(classifier_paddle([\"Sarcastic sentence one.\", \"Sarcastic sentence two\"]))","metadata":{"id":"332cVgE7r6p7","outputId":"ecdecc01-f05a-4da0-b001-cb0b7574520b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking if the accuracy metric works.","metadata":{"id":"iyWPt9Nmaf2s"}},{"cell_type":"code","source":"predicts = paddle.to_tensor(np.array([[0.60146040, 0.39853954],\n        [0.63550186, 0.36449814],\n        [0.67369622, 0.32630381],\n        [0.61961246, 0.38038763],\n        [0.64124215, 0.35875788],\n        [0.72334731, 0.27665269],\n        [0.60772324, 0.39227673],\n        [0.68578976, 0.31421021],\n        ]))\ny_data = paddle.to_tensor(np.array([[0], [0], [0], [0], [1], [1], [0], [1]]))\naccuracy = paddle.metric.accuracy(predicts, y_data)\nm = paddle.metric.Accuracy()\ncorrect = m.compute(predicts, y_data)\nm.update(correct)\nres = m.accumulate()\nprint(res, accuracy)","metadata":{"id":"nGEM3rv6D1uV","outputId":"dac75949-9a6d-424f-8826-f0a1bfa300d8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logs, trained_classifier_paddle = train(classifier_paddle)","metadata":{"id":"XPlB6Cru0B6B","outputId":"853db9b0-0e06-400d-86cf-1f300cc0aeaf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BUILDING LSTM ON CORE IVY","metadata":{"id":"Vy7iZ8QnAS2-"}},{"cell_type":"markdown","source":"The training of the BERT model is computationally fairly expensive. It might be better to prepare your own model, using core Ivy on torch or jax backend.","metadata":{"id":"MlrC62x1gvI1"}},{"cell_type":"code","source":"# dir(tokenizer)","metadata":{"id":"SEPdRlMT6Ybd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.vocab_size)\nprint(tokenizer.all_special_tokens_extended)\nprint(tokenizer.all_special_ids)\nprint(tokenizer.pad_token_id)","metadata":{"id":"mKyWQmUR6grC","outputId":"72aaf16b-1d93-46ee-d305-529cdf4982ae"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = list(df.sample(8)[\"title\"])\nprint(sample)\ntokenizer(sample, add_special_tokens=True, padding=True, truncation=True)","metadata":{"id":"_jEnXjjUOo7f","outputId":"3b7e5446-cd75-4c2e-d53b-3bad6852d87b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ivy.set_backend(\"torch\")\nnum_embeddings = tokenizer.vocab_size\nembedding_dim = 5\npad_token_id = tokenizer.pad_token_id\ninput_channels = embedding_dim\nnum_classes = 2\noutput_channels = 1\nnum_layers = 1\nlinear_input_channels = 2\nmax_length = 29\ntokenizer.model_max_length = max_length\neps = 1e-05\ntesting_input = list(df.sample(8)[\"title\"])\nbatch_size = 8\nlinear_input_channels = (tokenizer.model_max_length + 3) * batch_size # 3 comes from the hidden states of the LSTM\nlinear_output_channels = num_classes * batch_size\nnormalized_shape = (num_classes)\n\nclass LSTM_postproc(Module):\n\n  def __init__(self):\n    super(LSTM_postproc, self).__init__()\n\n  def __call__(self, args):\n\n    lstm_output, lstm_state = args\n    lstm_state_latest, lstm_state_hidden = lstm_state\n    lstm_state_latest = ivy.array(lstm_state_latest)\n    # print(lstm_state_hidden, lstm_state_latest)\n    lstm_state_hidden = ivy.array([state for state in lstm_state_hidden][0])\n\n    lstm_state = ivy.concat((lstm_state_latest, lstm_state_hidden), axis=0).reshape((batch_size, -1, 1))\n    # print(lstm_output.shape, lstm_state.shape)\n    out = ivy.concat([lstm_output, lstm_state], axis=1)\n    out = out.flatten()\n    return out\n\nclass Tokenizer(Module):\n\n  def __init__(self, tokenizer):\n    super(Tokenizer, self).__init__()\n    self.tokenizer = tokenizer\n\n  def __call__(self, args):\n    args = list(args)\n    return self.tokenizer(args, add_special_tokens=True, max_length=max_length, padding=\"max_length\", truncation=True)[\"input_ids\"]\n\nclass Reshaper(Module):\n\n  def __init__(self):\n    super(Reshaper, self).__init__()\n\n  def __call__(self, args):\n    return args.reshape((batch_size, num_classes))\n\nivy_LSTM = Sequential(\n    Tokenizer(tokenizer),\n    Embedding(num_embeddings, embedding_dim, pad_token_id),\n    LSTM(input_channels, output_channels, num_layers=1, return_sequence=True, return_state=True, device=None, v=None, dtype=None),\n    LSTM_postproc(),\n    Linear(linear_input_channels, linear_output_channels, with_bias=True),\n    Reshaper(),\n    Sigmoid(),\n    Softmax(),\n)","metadata":{"id":"d_uPnXJd7flG","outputId":"179d3094-7c3a-4cbd-b05b-0fcfc18c306a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dir)","metadata":{"id":"7Igom39EGNt8","outputId":"ff871a1e-4557-4024-c3c1-353f4ad3accd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ivy_train_loader(dataset = df, batch_size = 8):\n  num_batches = int(len(dataset)/batch_size)\n  out = ((dataset[\"title\"][batch_idx * batch_size : batch_idx * batch_size + batch_size], dataset[\"label\"][batch_idx * batch_size : batch_idx * batch_size + batch_size]) for batch_idx in range(num_batches))\n  return out\n\nloader = ivy_train_loader()\nfor batch_id, data in tqdm(enumerate(loader)):\n    x_data = data[0]\n    y_data = data[1]\n    ivy_LSTM_test_out = ivy_LSTM(x_data)\n    # print()\n    # print(ivy.sum(ivy_LSTM_test_out, axis=1))\n    if batch_id == 4:\n      break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_hot(args, num_clases = 2):\n  out = [[1 if idx == elem else 0 for idx in range(2)] for elem in args]\n  return out\n\ndef argmax(args):\n  out = [ivy.argmax(elem) for elem in args]\n  return out\n\nprint(one_hot([0, 0, 1, 0]))\nprint(argmax(ivy.array([[0.49967843, 0.50032151],\n       [0.49986687, 0.50013322],\n       [0.49912587, 0.50087422],\n       [0.50080854, 0.4991914 ],\n       [0.50049627, 0.4995037 ],\n       [0.4998956 , 0.50010443],\n       [0.50008798, 0.49991205],\n       [0.50053447, 0.49946556]])))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ivy.set_backend(\"torch\")\nnum_embeddings = tokenizer.vocab_size\nembedding_dim = 5\npad_token_id = tokenizer.pad_token_id\ninput_channels = embedding_dim\nnum_classes = 2\noutput_channels = 1\nnum_layers = 1\nlinear_input_channels = 2\nmax_length = 29\ntokenizer.model_max_length = max_length\neps = 1e-05\ntesting_input = list(df.sample(8)[\"title\"])\nbatch_size = 8\nlinear_input_channels = (tokenizer.model_max_length + 3) * batch_size # 3 comes from the hidden states of the LSTM\nlinear_output_channels = num_classes * batch_size\nnormalized_shape = (num_classes)\n\nclass LSTM_postproc(Module):\n\n  def __init__(self):\n    super(LSTM_postproc, self).__init__()\n\n  def __call__(self, args):\n\n    lstm_output, lstm_state = args\n    lstm_state_latest, lstm_state_hidden = lstm_state\n    lstm_state_latest = ivy.array(lstm_state_latest)\n    # print(lstm_state_hidden, lstm_state_latest)\n    lstm_state_hidden = ivy.array([state for state in lstm_state_hidden][0])\n\n    lstm_state = ivy.concat((lstm_state_latest, lstm_state_hidden), axis=0).reshape((batch_size, -1, 1))\n    # print(lstm_output.shape, lstm_state.shape)\n    out = ivy.concat([lstm_output, lstm_state], axis=1)\n    out = out.flatten()\n    return out\n\nclass Tokenizer(Module):\n\n  def __init__(self, tokenizer):\n    super(Tokenizer, self).__init__()\n    self.tokenizer = tokenizer\n\n  def __call__(self, args):\n    args = list(args)\n    return self.tokenizer(args, add_special_tokens=True, max_length=max_length, padding=\"max_length\", truncation=True)[\"input_ids\"]\n\nclass Reshaper(Module):\n\n  def __init__(self):\n    super(Reshaper, self).__init__()\n\n  def __call__(self, args):\n    return args.reshape((batch_size, num_classes))\n\nclass Argmax(Module):\n\n  def __init__(self):\n    super(Argmax, self).__init__()\n\n  def __call__(self, args):\n    return ivy.argmax(args, axis=-1)\n\n\n\nivy_LSTM = Sequential(\n    Tokenizer(tokenizer),\n    Embedding(num_embeddings, embedding_dim, pad_token_id),\n    LSTM(input_channels, output_channels, num_layers=1, return_sequence=True, return_state=True, device=None, v=None, dtype=None),\n    LSTM_postproc(),\n    Linear(linear_input_channels, linear_output_channels, with_bias=True),\n    Reshaper(),\n    Sigmoid(),\n    Softmax(),\n    Argmax(),\n)","metadata":{"id":"4dQ2Recq9Xo8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_ivy(model):\n  logs = []\n  learning_rate = 3e-5\n  opt = SGD(lr=learning_rate, inplace=True, stop_gradients=True, trace_on_next_step=False)\n  loss_fn = CrossEntropyLoss(axis=-1, epsilon=1e-07, reduction='sum')\n  epochs = 2\n  grads = ivy.zeros_like(model.v)\n  classifier = model\n  train_loader = ivy_train_loader(df, batch_size)\n\n  for epoch in range(epochs):\n\n    for batch_id, data in tqdm(enumerate(train_loader)):\n\n      x_data = data[0]\n      y_data = list(data[1])\n      # print(y_data)\n      # The transpiled model seems to have problems with inputs, so instead of feeding it a container, we map onto one.\n      predictions = classifier(x_data)\n\n      loss = loss_fn(predictions, y_data).float()\n      loss.requires_grad = True\n      # print(f\"LOSS: {loss}\")\n      \n      # acc = paddle.metric.accuracy(predicts, y_data) # This needs to be corrected.\n      loss.backward()\n\n      # update parameters\n      opt.step(model.v, grads)\n\n      if batch_id % 100 == 0:\n          # print(\"\\nepoch: {}, batch_id: {}, loss is: {}, acc is: {}\".format(epoch, batch_id, loss.numpy(), acc))\n          logs.append([epoch, batch_id, loss])\n\n      # opt.clear_grad()\n    gc.collect()\n\n\n  obj = {'model': model.state_dict(), 'opt': opt.state_dict(), 'epoch': epochs}\n  path = '/content/demos/Contributor_demos/Sarcasm Detection/model.pdparams'\n  paddle.save(obj, path)\n\n  return logs, model","metadata":{"id":"1NqBJx51TYSd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logs, model = train_ivy(ivy_LSTM)","metadata":{},"execution_count":null,"outputs":[]}]}