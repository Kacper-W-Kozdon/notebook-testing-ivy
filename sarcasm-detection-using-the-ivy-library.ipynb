{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":36545,"sourceType":"datasetVersion","datasetId":1309}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kacperkodo/sarcasm-detection-using-the-ivy-library?scriptVersionId=171047728\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# DEPENDANCIES AND SETUP","metadata":{"id":"s2B-C0ETR8j-"}},{"cell_type":"markdown","source":"Installing kaggle and uploading the API key necessary to use it.","metadata":{"id":"lVY3Z4myS1O4"}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-04-08T19:35:09.173651Z","iopub.execute_input":"2024-04-08T19:35:09.174118Z","iopub.status.idle":"2024-04-08T19:35:11.161271Z","shell.execute_reply.started":"2024-04-08T19:35:09.174075Z","shell.execute_reply":"2024-04-08T19:35:11.160125Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/input/sarcasm/train-balanced-sarc.csv.gz\n/kaggle/input/sarcasm/train-balanced-sarcasm.csv\n/kaggle/input/sarcasm/test-balanced.csv\n/kaggle/input/sarcasm/test-unbalanced.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q kaggle\n# from google.colab import files\n# from google.colab import userdata\nimport os\n# files.upload(); #Upload kaggle.json - you can get from the kaggle account settings, from the API section.","metadata":{"id":"7R4luV8tSDFn","outputId":"458b86d8-f64a-4581-aa23-5518e62f0623","execution":{"iopub.status.busy":"2024-04-08T19:35:11.163182Z","iopub.execute_input":"2024-04-08T19:35:11.163699Z","iopub.status.idle":"2024-04-08T19:35:27.668952Z","shell.execute_reply.started":"2024-04-08T19:35:11.163649Z","shell.execute_reply":"2024-04-08T19:35:27.667396Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Installing packages necessary to use torch's transformers.","metadata":{"id":"GRyxyRkNqONt"}},{"cell_type":"code","source":"!pip install tqdm boto3 requests regex sentencepiece sacremoses botocore>=1.34.79","metadata":{"id":"yhD653HGqOj2","outputId":"4408500f-9c76-401e-adb7-05b71e5fa2f4","execution":{"iopub.status.busy":"2024-04-08T19:36:19.654116Z","iopub.execute_input":"2024-04-08T19:36:19.6545Z","iopub.status.idle":"2024-04-08T19:36:40.533443Z","shell.execute_reply.started":"2024-04-08T19:36:19.654447Z","shell.execute_reply":"2024-04-08T19:36:40.532087Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.12.1 requires botocore<1.34.52,>=1.34.41, but you have botocore 1.29.165 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"To use the API, credentials need to be copied into the kaggle folder. If everything works, the output will show the list of available datasets.","metadata":{"id":"aCN2c1DGTbVM"}},{"cell_type":"markdown","source":"","metadata":{"id":"P1aQHs-9Tkt2"}},{"cell_type":"markdown","source":"Preparing the ivy library.","metadata":{"id":"_Sf8EImZT6kZ"}},{"cell_type":"code","source":"#Insert the correct user when cloning the repos. Make sure that they are up-to-date.\n\n!git clone \"https://github.com/Kacper-W-Kozdon/demos.git\"\n# !git clone \"https://github.com/Kacper-W-Kozdon/ivy.git\"\n!pip install -U -q paddlepaddle ivy accelerate>=0.21.0  2>/dev/null # If ran in a notebook with only cpu enabled, edit \"paddlepaddle-gpu\" to \"paddlepaddle\"","metadata":{"id":"7DMn3EoEUBGQ","outputId":"357f988a-3c36-4b74-a10d-ff233047b17c","execution":{"iopub.status.busy":"2024-04-08T19:36:40.536751Z","iopub.execute_input":"2024-04-08T19:36:40.53723Z","iopub.status.idle":"2024-04-08T19:37:08.090673Z","shell.execute_reply.started":"2024-04-08T19:36:40.537183Z","shell.execute_reply":"2024-04-08T19:37:08.089327Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"fatal: destination path 'demos' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Next: import the ivy library and get the dataset.","metadata":{"id":"y1sA3gFuWjDE"}},{"cell_type":"code","source":"import ivy","metadata":{"id":"_NUgteS_Dluc","execution":{"iopub.status.busy":"2024-04-08T19:37:08.092773Z","iopub.execute_input":"2024-04-08T19:37:08.093136Z","iopub.status.idle":"2024-04-08T19:37:08.919682Z","shell.execute_reply.started":"2024-04-08T19:37:08.093103Z","shell.execute_reply":"2024-04-08T19:37:08.91855Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Import the libraries suggested in the model which is to be transpiled.","metadata":{"id":"UKN-VX8QXDEG"}},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd  # For data manipulation and analysis\nimport gc  # For garbage collection to manage memory\nimport re  # For regular expressions\nimport numpy as np  # For numerical operations and arrays\nimport tensorflow as tf\nimport torch  # PyTorch library for deep learning\nimport paddle","metadata":{"execution":{"iopub.status.busy":"2024-04-08T19:37:39.881795Z","iopub.execute_input":"2024-04-08T19:37:39.882458Z","iopub.status.idle":"2024-04-08T19:38:09.209363Z","shell.execute_reply.started":"2024-04-08T19:37:39.882426Z","shell.execute_reply":"2024-04-08T19:38:09.208339Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"2024-04-08 19:37:43.380611: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-08 19:37:43.380786: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-08 19:37:43.639231: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\nimport ivy.functional.frontends.paddle as paddle_frontend\n\n# Libraries to accompany torch's transformers\nimport tqdm\nimport boto3\nimport requests\nimport regex\nimport sentencepiece\nimport sacremoses\n\nimport warnings  # For handling warnings\nwarnings.filterwarnings(\"ignore\")  # Ignore warning messages\n\nfrom transformers import AutoModel, AutoTokenizer  # Transformers library for natural language processing\n# from transformers import TextDataset, LineByLineTextDataset, DataCollatorForLanguageModeling, \\\n# pipeline, Trainer, TrainingArguments, DataCollatorWithPadding  # Transformers components for text processing\nfrom transformers import TextDataset, LineByLineTextDataset, DataCollatorForLanguageModeling, \\\npipeline, TrainingArguments, DataCollatorWithPadding\nfrom transformers import AutoModelForSequenceClassification  # Transformer model for sequence classification\n\nimport accelerate\n\n# from nlp import Dataset  # Import custom 'Dataset' class for natural language processing tasks\nfrom imblearn.over_sampling import RandomOverSampler  # For oversampling to handle class imbalance\n# import datasets  # Import datasets library\n# from datasets import Dataset, Image, ClassLabel  # Import custom 'Dataset', 'ClassLabel', and 'Image' classes\nfrom transformers import pipeline  # Transformers library for pipelines\nfrom bs4 import BeautifulSoup  # For parsing HTML content\n\nimport matplotlib.pyplot as plt  # For data visualization\nimport itertools  # For working with iterators\nfrom sklearn.metrics import (  # Import various metrics from scikit-learn\n    accuracy_score,  # For calculating accuracy\n    roc_auc_score,  # For ROC AUC score\n    confusion_matrix,  # For confusion matrix\n    classification_report,  # For classification report\n    f1_score  # For F1 score\n)\n\n# from datasets import load_metric  # Import load_metric function to load evaluation metrics\n\nfrom tqdm import tqdm  # For displaying progress bars\n\ntqdm.pandas()  # Enable progress bars for pandas operations","metadata":{"id":"19rgBXHJXHFu","execution":{"iopub.status.busy":"2024-04-08T19:38:15.504524Z","iopub.execute_input":"2024-04-08T19:38:15.505945Z","iopub.status.idle":"2024-04-08T19:38:26.07685Z","shell.execute_reply.started":"2024-04-08T19:38:15.505905Z","shell.execute_reply":"2024-04-08T19:38:26.075783Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"device = \"gpu:0\" if paddle.device.cuda.device_count() else \"cpu\" # Either \"gpu\" or \"gpu:0\".\nivy.set_default_device(device)\nivy.set_soft_device_mode(True)\n","metadata":{"id":"bXr9tGFLGRPI","execution":{"iopub.status.busy":"2024-04-08T19:38:33.334975Z","iopub.execute_input":"2024-04-08T19:38:33.336238Z","iopub.status.idle":"2024-04-08T19:38:33.344039Z","shell.execute_reply.started":"2024-04-08T19:38:33.336195Z","shell.execute_reply":"2024-04-08T19:38:33.342808Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(ivy.default_device())\nprint(ivy.num_gpus())\nprint(torch.cuda.is_available())","metadata":{"id":"ijs6fSKL9QZ4","outputId":"23f84802-981c-4869-ea4c-c9a94489e2ff","execution":{"iopub.status.busy":"2024-04-08T19:39:02.646997Z","iopub.execute_input":"2024-04-08T19:39:02.647941Z","iopub.status.idle":"2024-04-08T19:39:02.711401Z","shell.execute_reply.started":"2024-04-08T19:39:02.647906Z","shell.execute_reply":"2024-04-08T19:39:02.710031Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"cpu\n0\nTrue\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Set the seeds.","metadata":{"id":"JU7qbxYdsVlK"}},{"cell_type":"code","source":"tf.keras.utils.set_random_seed(0)\ntorch.manual_seed(0)\npaddle.seed(0)","metadata":{"id":"HxD1xridsU_l","outputId":"1b43d30a-d4bb-401b-f540-988ccbc9b24a","execution":{"iopub.status.busy":"2024-04-08T19:39:12.714623Z","iopub.execute_input":"2024-04-08T19:39:12.715498Z","iopub.status.idle":"2024-04-08T19:39:12.728523Z","shell.execute_reply.started":"2024-04-08T19:39:12.715447Z","shell.execute_reply":"2024-04-08T19:39:12.727327Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<paddle.base.libpaddle.Generator at 0x7fd1d36b0e70>"},"metadata":{}}]},{"cell_type":"markdown","source":"Get the API key for ivy transpiler from your account and upload it to the project. Move it to the correct directory.","metadata":{"id":"zwU4oNrkXyxT"}},{"cell_type":"code","source":"pwd","metadata":{"execution":{"iopub.status.busy":"2024-04-08T19:39:17.904392Z","iopub.execute_input":"2024-04-08T19:39:17.904857Z","iopub.status.idle":"2024-04-08T19:39:17.911655Z","shell.execute_reply.started":"2024-04-08T19:39:17.904827Z","shell.execute_reply":"2024-04-08T19:39:17.910532Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}]},{"cell_type":"code","source":"# files.upload(); #Upload key.pem - you can get from the kaggle account settings, from the API section.\n\nfrom kaggle_secrets import UserSecretsClient\nsecret_label = \"key.pem\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\n\nwith open('/kaggle/working/key.pem','w+') as ivy_api_key:\n    ivy_api_key.write(secret_value)\n\n","metadata":{"id":"LME9LyKaXyVF","outputId":"8568ff9c-19d6-4947-fba8-77670684d6c6","execution":{"iopub.status.busy":"2024-04-08T19:39:24.605955Z","iopub.execute_input":"2024-04-08T19:39:24.606725Z","iopub.status.idle":"2024-04-08T19:39:24.760934Z","shell.execute_reply.started":"2024-04-08T19:39:24.606688Z","shell.execute_reply":"2024-04-08T19:39:24.759737Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/.ivy #It might be necessary to change \".ivy\" to \"ivy\".\n!cp key.pem /kaggle/working/.ivy","metadata":{"id":"NbuJUunHYIwg","execution":{"iopub.status.busy":"2024-04-08T19:43:00.45772Z","iopub.execute_input":"2024-04-08T19:43:00.458116Z","iopub.status.idle":"2024-04-08T19:43:02.743385Z","shell.execute_reply.started":"2024-04-08T19:43:00.458087Z","shell.execute_reply":"2024-04-08T19:43:02.741796Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"mkdir: cannot create directory '/kaggle/working/.ivy': File exists\n","output_type":"stream"}]},{"cell_type":"markdown","source":"First we're loading the tokenizer and the model from torch. All of the basic set-up instructions can be found here: https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/huggingface_pytorch-transformers.ipynb#scrollTo=72d8f2de","metadata":{"id":"DWCnfosUshGK"}},{"cell_type":"code","source":"tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\nmodel = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\n\nsequence_classifier = torch.hub.load('huggingface/pytorch-transformers', 'modelForSequenceClassification', 'bert-base-cased')","metadata":{"id":"2rZY3rhisgXZ","outputId":"6737a6a8-76a2-4881-dcaa-1e2fa12372b4","execution":{"iopub.status.busy":"2024-04-08T19:39:39.074001Z","iopub.execute_input":"2024-04-08T19:39:39.075334Z","iopub.status.idle":"2024-04-08T19:39:49.664736Z","shell.execute_reply.started":"2024-04-08T19:39:39.075288Z","shell.execute_reply":"2024-04-08T19:39:49.663494Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Downloading: \"https://github.com/huggingface/pytorch-transformers/zipball/main\" to /root/.cache/torch/hub/main.zip\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7687ad625194e3fa39c1729bf8cbbee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8183ba32bb84e4b94395d009fcc8f1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64a4d33a768041418ab8f1ed2a355d38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeab0fe4252f406f81750c1eead212fb"}},"metadata":{}},{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24911e6f295e4eef93f40296f22134f8"}},"metadata":{}},{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from ivy.stateful.module import Module\nfrom ivy.stateful.sequential import Sequential\nfrom ivy.stateful.layers import *\nfrom ivy.stateful.losses import *\nfrom ivy.stateful.optimizers import *\nfrom ivy.stateful.activations import *\nfrom ivy.stateful.initializers import *\nfrom ivy.stateful.norms import *\n","metadata":{"id":"b0MdcERZYS_6","execution":{"iopub.status.busy":"2024-04-08T19:39:52.360013Z","iopub.execute_input":"2024-04-08T19:39:52.360915Z","iopub.status.idle":"2024-04-08T19:39:52.367279Z","shell.execute_reply.started":"2024-04-08T19:39:52.360879Z","shell.execute_reply":"2024-04-08T19:39:52.366093Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# df = pd.read_csv(\"/content/demos/Contributor_demos/Sarcasm Detection/train-balanced-sarcasm.csv\")\ndf = pd.read_csv(\"/kaggle/input/sarcasm/train-balanced-sarcasm.csv\")\ndf = df.drop_duplicates()\ndf = df.rename(columns={'comment': 'title'})\ndf = df[['label', 'title']]\ndf = df[~df['label'].isnull()]\ndf = df[~df['title'].isnull()]\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T19:45:30.06354Z","iopub.execute_input":"2024-04-08T19:45:30.064419Z","iopub.status.idle":"2024-04-08T19:45:38.56385Z","shell.execute_reply.started":"2024-04-08T19:45:30.064382Z","shell.execute_reply":"2024-04-08T19:45:38.562719Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"        label                                              title\n627321      0      How do people keep getting matches on Tinder?\n378524      0  Same thing happened to me, so it's definitely ...\n96238       0  I agree with you, but too much of a good thing...\n598116      0                         'Twas a joke I do believe.\n217680      0  [M] it will get bigger depending on luck of th...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>627321</th>\n      <td>0</td>\n      <td>How do people keep getting matches on Tinder?</td>\n    </tr>\n    <tr>\n      <th>378524</th>\n      <td>0</td>\n      <td>Same thing happened to me, so it's definitely ...</td>\n    </tr>\n    <tr>\n      <th>96238</th>\n      <td>0</td>\n      <td>I agree with you, but too much of a good thing...</td>\n    </tr>\n    <tr>\n      <th>598116</th>\n      <td>0</td>\n      <td>'Twas a joke I do believe.</td>\n    </tr>\n    <tr>\n      <th>217680</th>\n      <td>0</td>\n      <td>[M] it will get bigger depending on luck of th...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# DATASET AND MODEL OVERVIEW","metadata":{"id":"bXFPiT6SgPob"}},{"cell_type":"code","source":"!echo -n API_KEY > .ivy/key.pem","metadata":{"execution":{"iopub.status.busy":"2024-04-08T19:43:58.298258Z","iopub.execute_input":"2024-04-08T19:43:58.298751Z","iopub.status.idle":"2024-04-08T19:43:59.419783Z","shell.execute_reply.started":"2024-04-08T19:43:58.298717Z","shell.execute_reply":"2024-04-08T19:43:59.418461Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"/bin/bash: .ivy/key.pem: Not a directory\n","output_type":"stream"}]},{"cell_type":"code","source":"def count_words(text: str) -> int:\n  return len(text.split())\n\ndef count_symbols(text: str) -> int:\n  return len(\"\".join(text.split()))\n\ndef symbol_to_word_ratio(text: str) -> float:\n  return count_symbols(text)/count_words(text)\n\ndef upper_lower_ratio(text: str) -> float:\n  text = \"\".join(text.split())\n  return sum(1 for c in text if c.isupper())/(max([sum(1 for c in text if c.islower()), 1]))\n\ndf['word_count'] = df[\"title\"].apply(count_words)\ndf['symbol_count'] = df[\"title\"].apply(count_symbols)\ndf[\"upper_lower_ratio\"] = df[\"title\"].apply(upper_lower_ratio)\ndf[\"symbol_to_word_ratio\"] = df[\"title\"].apply(symbol_to_word_ratio)\ndf.sample(5)","metadata":{"id":"BVA6U5Y0c7vg","outputId":"13606496-e191-4005-a04a-4e516875bdfa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A few plots to see some some characteristics of the data.","metadata":{"id":"YY9ru5DbJqxa"}},{"cell_type":"code","source":"df_no_sarc = df.where(df[\"label\"] == 0)\ndf_no_sarc = df_no_sarc.where(df_no_sarc[\"word_count\"] <= 51)\ndf_sarc = df.where(df[\"label\"] == 1)\ndf_sarc = df_sarc.where(df_sarc[\"word_count\"] <= 51)\ndf_no_sarc = df_no_sarc[np.isfinite(df_no_sarc[\"word_count\"])]\ndf_sarc = df_sarc[np.isfinite(df_sarc[\"word_count\"])]\nplt.style.use('_mpl-gallery-nogrid')\n\nhist_df_no_sarc, bin_edges_no = np.histogram(df_no_sarc[\"word_count\"].values, density=True)\nhist_df_sarc, bin_edges = np.histogram(df_sarc[\"word_count\"].values, density=True)\n# plot:\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nbin_mids_no = [(bin_edges_no[i+1] + bin_edges_no[i])/2 for i in range(len(bin_edges_no) - 1)]\nbin_mids = [(bin_edges[i+1] + bin_edges[i])/2 for i in range(len(bin_edges) - 1)]\nax1.bar(bin_mids_no, hist_df_no_sarc, width=bin_edges_no[1] - bin_edges_no[0])\nax2.bar(bin_mids, hist_df_sarc, width=bin_edges[1] - bin_edges[0])\nax1.set_title(\"Hist no sarcasm\")\nax1.set_ylabel(\"density\")\nax1.set_xlabel(\"word count\")\nax1.set_xticks(bin_edges_no)\nax1.grid(True)\nax2.set_title(\"Hist sarcasm\")\nax2.set_xlabel(\"word count\")\nax2.set_xticks(bin_edges)\nax2.grid(True)\nplt.show()","metadata":{"id":"_LEHSFedgIBq","outputId":"f6f7cb26-7383-4fa9-f454-b7f07adba063","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_no_sarc = df.where(df[\"label\"] == 0)\ndf_no_sarc = df_no_sarc.where(df_no_sarc[\"symbol_count\"] <= 201)\ndf_sarc = df.where(df[\"label\"] == 1)\ndf_sarc = df_sarc.where(df_sarc[\"symbol_count\"] <= 201)\ndf_no_sarc = df_no_sarc[np.isfinite(df_no_sarc[\"symbol_count\"])]\ndf_sarc = df_sarc[np.isfinite(df_sarc[\"symbol_count\"])]\nplt.style.use('_mpl-gallery-nogrid')\n\nhist_df_no_sarc, bin_edges_no = np.histogram(df_no_sarc[\"symbol_count\"].values, density=True)\nhist_df_sarc, bin_edges = np.histogram(df_sarc[\"symbol_count\"].values, density=True)\n# plot:\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nbin_mids_no = [(bin_edges_no[i+1] + bin_edges_no[i])/2 for i in range(len(bin_edges_no) - 1)]\nbin_mids = [(bin_edges[i+1] + bin_edges[i])/2 for i in range(len(bin_edges) - 1)]\nax1.bar(bin_mids_no, hist_df_no_sarc, width=bin_edges_no[1] - bin_edges_no[0])\nax2.bar(bin_mids, hist_df_sarc, width=bin_edges[1] - bin_edges[0])\nax1.set_title(\"Hist no sarcasm\")\nax1.set_ylabel(\"density\")\nax1.set_xlabel(\"symbol count\")\nax1.set_xticks(bin_edges_no)\nax1.grid(True)\nax2.set_title(\"Hist sarcasm\")\nax2.set_xlabel(\"symbol count\")\nax2.set_xticks(bin_edges)\nax2.grid(True)\nplt.show()","metadata":{"id":"RcYhYzfygLc9","outputId":"df9ed393-4d87-4431-b393-76729e77cb97","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_no_sarc = df.where(df[\"label\"] == 0)\ndf_no_sarc = df_no_sarc.where(df_no_sarc[\"upper_lower_ratio\"] <= 0.3)\ndf_sarc = df.where(df[\"label\"] == 1)\ndf_sarc = df_sarc.where(df_sarc[\"upper_lower_ratio\"] <= 0.3)\ndf_no_sarc = df_no_sarc[np.isfinite(df_no_sarc[\"upper_lower_ratio\"])]\ndf_sarc = df_sarc[np.isfinite(df_sarc[\"upper_lower_ratio\"])]\nplt.style.use('_mpl-gallery-nogrid')\n\nhist_df_no_sarc, bin_edges_no = np.histogram(df_no_sarc[\"upper_lower_ratio\"].values, density=True)\nhist_df_sarc, bin_edges = np.histogram(df_sarc[\"upper_lower_ratio\"].values, density=True)\n# plot:\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nbin_mids_no = [(bin_edges_no[i+1] + bin_edges_no[i])/2 for i in range(len(bin_edges_no) - 1)]\nbin_mids = [(bin_edges[i+1] + bin_edges[i])/2 for i in range(len(bin_edges) - 1)]\nax1.bar(bin_mids_no, hist_df_no_sarc, width=bin_edges_no[1] - bin_edges_no[0])\nax2.bar(bin_mids, hist_df_sarc, width=bin_edges[1] - bin_edges[0])\nax1.set_title(\"Hist no sarcasm\")\nax1.set_ylabel(\"density\")\nax1.set_xlabel(\"upper/lower ratio\")\nax1.set_xticks(bin_edges_no)\nax1.grid(True)\nax2.set_title(\"Hist sarcasm\")\nax2.set_xlabel(\"upper/lower ratio\")\nax2.set_xticks(bin_edges)\nax2.grid(True)\nplt.show()","metadata":{"id":"vvKbuhLaDaSP","outputId":"f7756960-1a3d-4ace-b6a0-8addf8b6e53c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_no_sarc = df.where(df[\"label\"] == 0)\ndf_no_sarc = df_no_sarc.where(df_no_sarc[\"symbol_to_word_ratio\"] <= 11)\ndf_sarc = df.where(df[\"label\"] == 1)\ndf_sarc = df_sarc.where(df_sarc[\"symbol_to_word_ratio\"] <= 11)\ndf_no_sarc = df_no_sarc[np.isfinite(df_no_sarc[\"symbol_to_word_ratio\"])]\ndf_sarc = df_sarc[np.isfinite(df_sarc[\"symbol_to_word_ratio\"])]\nplt.style.use('_mpl-gallery-nogrid')\n\nhist_df_no_sarc, bin_edges_no = np.histogram(df_no_sarc[\"symbol_to_word_ratio\"].values, density=True)\nhist_df_sarc, bin_edges = np.histogram(df_sarc[\"symbol_to_word_ratio\"].values, density=True)\n# plot:\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nbin_mids_no = [(bin_edges_no[i+1] + bin_edges_no[i])/2 for i in range(len(bin_edges_no) - 1)]\nbin_mids = [(bin_edges[i+1] + bin_edges[i])/2 for i in range(len(bin_edges) - 1)]\nax1.bar(bin_mids_no, hist_df_no_sarc, width=bin_edges_no[1] - bin_edges_no[0])\nax2.bar(bin_mids, hist_df_sarc, width=bin_edges[1] - bin_edges[0])\nax1.set_title(\"Hist no sarcasm\")\nax1.set_ylabel(\"density\")\nax1.set_xlabel(\"symbols/words ratio\")\nax1.set_xticks(bin_edges_no)\nax1.grid(True)\nax2.set_title(\"Hist sarcasm\")\nax2.set_xlabel(\"symbols/words ratio\")\nax2.set_xticks(bin_edges)\nax2.grid(True)\nplt.show()","metadata":{"id":"HkuIzb1JF1U1","outputId":"7ab4df1a-bd8e-4188-d65b-f7d7c5075c17","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"id":"nk3vK1u3FlZc","outputId":"75d850b4-d36e-4f97-984c-92ef21501d3a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BUILDING LSTM ON CORE IVY","metadata":{"id":"Vy7iZ8QnAS2-"}},{"cell_type":"markdown","source":"The training of the BERT model is computationally fairly expensive. It might be better to prepare your own model, using core Ivy on torch or jax backend.","metadata":{"id":"MlrC62x1gvI1"}},{"cell_type":"code","source":"# dir(tokenizer)","metadata":{"id":"SEPdRlMT6Ybd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.vocab_size)\nprint(tokenizer.all_special_tokens_extended)\nprint(tokenizer.all_special_ids)\nprint(tokenizer.pad_token_id)","metadata":{"id":"mKyWQmUR6grC","outputId":"72aaf16b-1d93-46ee-d305-529cdf4982ae","execution":{"iopub.status.busy":"2024-04-08T19:46:19.517639Z","iopub.execute_input":"2024-04-08T19:46:19.518508Z","iopub.status.idle":"2024-04-08T19:46:19.525152Z","shell.execute_reply.started":"2024-04-08T19:46:19.51845Z","shell.execute_reply":"2024-04-08T19:46:19.523946Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"28996\n['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n[100, 102, 0, 101, 103]\n0\n","output_type":"stream"}]},{"cell_type":"code","source":"sample = list(df.sample(8)[\"title\"])\nprint(sample)\ntokenizer(sample, add_special_tokens=True, padding=True, truncation=True)","metadata":{"id":"_jEnXjjUOo7f","outputId":"3b7e5446-cd75-4c2e-d53b-3bad6852d87b","execution":{"iopub.status.busy":"2024-04-08T19:46:26.127013Z","iopub.execute_input":"2024-04-08T19:46:26.129007Z","iopub.status.idle":"2024-04-08T19:46:26.186999Z","shell.execute_reply.started":"2024-04-08T19:46:26.128947Z","shell.execute_reply":"2024-04-08T19:46:26.185918Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"['Sarco has it in for Lowes', 'I really enjoyed the dailies when I lived there, but I love thunderstorms.', 'This is where knowing the difference between a visual effect and a special effect comes in handy.', '*she, you woman molester.', 'And here I am just masturbating.', 'Protesting is unAmerican.', 'Roosterteeth podcast', 'Just remember, feminism is working to solve these problems.']\n","output_type":"stream"},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [[101, 17784, 19878, 1186, 1144, 1122, 1107, 1111, 14830, 1116, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 146, 1541, 4927, 1103, 5358, 18575, 1279, 1165, 146, 2077, 1175, 117, 1133, 146, 1567, 17209, 18251, 1116, 119, 102], [101, 1188, 1110, 1187, 3650, 1103, 3719, 1206, 170, 5173, 2629, 1105, 170, 1957, 2629, 2502, 1107, 25997, 119, 102, 0], [101, 115, 1131, 117, 1128, 1590, 26365, 4648, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1262, 1303, 146, 1821, 1198, 18871, 2149, 14602, 1158, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 5096, 13053, 1158, 1110, 8362, 1592, 4027, 4578, 1179, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 155, 24163, 26032, 1582, 19777, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2066, 2676, 117, 25787, 1110, 1684, 1106, 9474, 1292, 2645, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}"},"metadata":{}}]},{"cell_type":"code","source":"ivy.set_backend(\"torch\")\nnum_embeddings = tokenizer.vocab_size\nembedding_dim = 5\npad_token_id = tokenizer.pad_token_id\ninput_channels = embedding_dim\nnum_classes = 2\noutput_channels = 1\nnum_layers = 1\nlinear_input_channels = 2\nmax_length = 29\ntokenizer.model_max_length = max_length\neps = 1e-05\ntesting_input = list(df.sample(8)[\"title\"])\nbatch_size = 8\nlinear_input_channels = (tokenizer.model_max_length + 3) * batch_size # 3 comes from the hidden states of the LSTM\nlinear_output_channels = num_classes * batch_size\nnormalized_shape = (num_classes)\n\nclass LSTM_postproc(Module):\n\n  def __init__(self):\n    super(LSTM_postproc, self).__init__()\n\n  def __call__(self, args):\n\n    lstm_output, lstm_state = args\n    lstm_state_latest, lstm_state_hidden = lstm_state\n    lstm_state_latest = ivy.array(lstm_state_latest)\n    # print(lstm_state_hidden, lstm_state_latest)\n    lstm_state_hidden = ivy.array([state for state in lstm_state_hidden][0])\n\n    lstm_state = ivy.concat((lstm_state_latest, lstm_state_hidden), axis=0).reshape((batch_size, -1, 1))\n    # print(lstm_output.shape, lstm_state.shape)\n    out = ivy.concat([lstm_output, lstm_state], axis=1)\n    out = out.flatten()\n    return out\n\nclass Tokenizer(Module):\n\n  def __init__(self, tokenizer):\n    super(Tokenizer, self).__init__()\n    self.tokenizer = tokenizer\n\n  def __call__(self, args):\n    args = list(args)\n    return self.tokenizer(args, add_special_tokens=True, max_length=max_length, padding=\"max_length\", truncation=True)[\"input_ids\"]\n\nclass Reshaper(Module):\n\n  def __init__(self):\n    super(Reshaper, self).__init__()\n\n  def __call__(self, args):\n    return args.reshape((batch_size, num_classes))\n\nivy_LSTM = Sequential(\n    Tokenizer(tokenizer),\n    Embedding(num_embeddings, embedding_dim, pad_token_id),\n    LSTM(input_channels, output_channels, num_layers=1, return_sequence=True, return_state=True, device=None, v=None, dtype=None),\n    LSTM_postproc(),\n    Linear(linear_input_channels, linear_output_channels, with_bias=True),\n    Reshaper(),\n    Sigmoid(),\n    Softmax(),\n)","metadata":{"id":"d_uPnXJd7flG","outputId":"179d3094-7c3a-4cbd-b05b-0fcfc18c306a","execution":{"iopub.status.busy":"2024-04-08T19:46:31.335309Z","iopub.execute_input":"2024-04-08T19:46:31.33573Z","iopub.status.idle":"2024-04-08T19:46:32.210911Z","shell.execute_reply.started":"2024-04-08T19:46:31.335699Z","shell.execute_reply":"2024-04-08T19:46:32.209827Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"print(dir)","metadata":{"id":"7Igom39EGNt8","outputId":"ff871a1e-4557-4024-c3c1-353f4ad3accd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ivy_train_loader(dataset = df, batch_size = 8):\n  num_batches = int(len(dataset)/batch_size)\n  out = ((dataset[\"title\"][batch_idx * batch_size : batch_idx * batch_size + batch_size], dataset[\"label\"][batch_idx * batch_size : batch_idx * batch_size + batch_size]) for batch_idx in range(num_batches))\n  return out\n\nloader = ivy_train_loader()\nfor batch_id, data in tqdm(enumerate(loader)):\n    x_data = data[0]\n    y_data = data[1]\n    ivy_LSTM_test_out = ivy_LSTM(x_data)\n    # print()\n    # print(ivy.sum(ivy_LSTM_test_out, axis=1))\n    if batch_id == 4:\n      break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_hot(args, num_clases = 2):\n  out = [[1 if idx == elem else 0 for idx in range(2)] for elem in args]\n  return out\n\ndef argmax(args):\n  out = [ivy.argmax(elem) for elem in args]\n  return out\n\nprint(one_hot([0, 0, 1, 0]))\nprint(argmax(ivy.array([[0.49967843, 0.50032151],\n       [0.49986687, 0.50013322],\n       [0.49912587, 0.50087422],\n       [0.50080854, 0.4991914 ],\n       [0.50049627, 0.4995037 ],\n       [0.4998956 , 0.50010443],\n       [0.50008798, 0.49991205],\n       [0.50053447, 0.49946556]])))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ivy.set_backend(\"torch\")\nnum_embeddings = tokenizer.vocab_size\nembedding_dim = 5\npad_token_id = tokenizer.pad_token_id\ninput_channels = embedding_dim\nnum_classes = 2\noutput_channels = 1\nnum_layers = 1\nlinear_input_channels = 2\nmax_length = 29\ntokenizer.model_max_length = max_length\neps = 1e-05\ntesting_input = list(df.sample(8)[\"title\"])\nbatch_size = 8\nlinear_input_channels = (tokenizer.model_max_length + 3) * batch_size # 3 comes from the hidden states of the LSTM\nlinear_output_channels = num_classes * batch_size\nnormalized_shape = (num_classes)\n\nclass LSTM_postproc(Module):\n\n  def __init__(self):\n    super(LSTM_postproc, self).__init__()\n\n  def __call__(self, args):\n\n    lstm_output, lstm_state = args\n    lstm_state_latest, lstm_state_hidden = lstm_state\n    lstm_state_latest = ivy.array(lstm_state_latest)\n    # print(lstm_state_hidden, lstm_state_latest)\n    lstm_state_hidden = ivy.array([state for state in lstm_state_hidden][0])\n\n    lstm_state = ivy.concat((lstm_state_latest, lstm_state_hidden), axis=0).reshape((batch_size, -1, 1))\n    # print(lstm_output.shape, lstm_state.shape)\n    out = ivy.concat([lstm_output, lstm_state], axis=1)\n    out = out.flatten()\n    return out\n\nclass Tokenizer(Module):\n\n  def __init__(self, tokenizer):\n    super(Tokenizer, self).__init__()\n    self.tokenizer = tokenizer\n\n  def __call__(self, args):\n    args = list(args)\n    return self.tokenizer(args, add_special_tokens=True, max_length=max_length, padding=\"max_length\", truncation=True)[\"input_ids\"]\n\nclass Reshaper(Module):\n\n  def __init__(self):\n    super(Reshaper, self).__init__()\n\n  def __call__(self, args):\n    return args.reshape((batch_size, num_classes))\n\nclass Argmax(Module):\n\n  def __init__(self):\n    super(Argmax, self).__init__()\n\n  def __call__(self, args):\n    return ivy.argmax(args, axis=-1)\n\n\n\nivy_LSTM = Sequential(\n    Tokenizer(tokenizer),\n    Embedding(num_embeddings, embedding_dim, pad_token_id),\n    LSTM(input_channels, output_channels, num_layers=1, return_sequence=True, return_state=True, device=None, v=None, dtype=None),\n    LSTM_postproc(),\n    Linear(linear_input_channels, linear_output_channels, with_bias=True),\n    Reshaper(),\n    Sigmoid(),\n    Softmax(),\n    Argmax(),\n)","metadata":{"id":"4dQ2Recq9Xo8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_ivy(model):\n  logs = []\n  learning_rate = 3e-5\n  opt = SGD(lr=learning_rate, inplace=True, stop_gradients=True, trace_on_next_step=False)\n  loss_fn = CrossEntropyLoss(axis=-1, epsilon=1e-07, reduction='sum')\n  epochs = 2\n  grads = ivy.zeros_like(model.v)\n  classifier = model\n  train_loader = ivy_train_loader(df, batch_size)\n\n  for epoch in range(epochs):\n\n    for batch_id, data in tqdm(enumerate(train_loader)):\n\n      x_data = data[0]\n      y_data = list(data[1])\n      # print(y_data)\n      # The transpiled model seems to have problems with inputs, so instead of feeding it a container, we map onto one.\n      predictions = classifier(x_data)\n\n      loss = loss_fn(predictions, y_data).float()\n      loss.requires_grad = True\n      # print(f\"LOSS: {loss}\")\n      \n      # acc = paddle.metric.accuracy(predicts, y_data) # This needs to be corrected.\n      loss.backward()\n\n      # update parameters\n      opt.step(model.v, grads)\n\n      if batch_id % 100 == 0:\n          # print(\"\\nepoch: {}, batch_id: {}, loss is: {}, acc is: {}\".format(epoch, batch_id, loss.numpy(), acc))\n          logs.append([epoch, batch_id, loss])\n\n      # opt.clear_grad()\n    gc.collect()\n\n\n  obj = {'model': model.state_dict(), 'opt': opt.state_dict(), 'epoch': epochs}\n  path = '/content/demos/Contributor_demos/Sarcasm Detection/model.pdparams'\n  paddle.save(obj, path)\n\n  return logs, model","metadata":{"id":"1NqBJx51TYSd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logs, model = train_ivy(ivy_LSTM)","metadata":{},"execution_count":null,"outputs":[]}]}