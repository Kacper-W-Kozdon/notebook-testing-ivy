{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":36545,"sourceType":"datasetVersion","datasetId":1309}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kacperkodo/sarcasm-detection-using-the-ivy-library?scriptVersionId=171051367\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# DEPENDANCIES AND SETUP","metadata":{"id":"s2B-C0ETR8j-"}},{"cell_type":"markdown","source":"Installing kaggle and uploading the API key necessary to use it.","metadata":{"id":"lVY3Z4myS1O4"}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q kaggle\n# from google.colab import files\n# from google.colab import userdata\nimport os\n# files.upload(); #Upload kaggle.json - you can get from the kaggle account settings, from the API section.","metadata":{"id":"7R4luV8tSDFn","outputId":"458b86d8-f64a-4581-aa23-5518e62f0623","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Installing packages necessary to use torch's transformers.","metadata":{"id":"GRyxyRkNqONt"}},{"cell_type":"code","source":"!pip install tqdm boto3 requests regex sentencepiece sacremoses botocore>=1.34.79","metadata":{"id":"yhD653HGqOj2","outputId":"4408500f-9c76-401e-adb7-05b71e5fa2f4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To use the API, credentials need to be copied into the kaggle folder. If everything works, the output will show the list of available datasets.","metadata":{"id":"aCN2c1DGTbVM"}},{"cell_type":"markdown","source":"","metadata":{"id":"P1aQHs-9Tkt2"}},{"cell_type":"markdown","source":"Preparing the ivy library.","metadata":{"id":"_Sf8EImZT6kZ"}},{"cell_type":"code","source":"#Insert the correct user when cloning the repos. Make sure that they are up-to-date.\n\n!git clone \"https://github.com/Kacper-W-Kozdon/demos.git\"\n# !git clone \"https://github.com/Kacper-W-Kozdon/ivy.git\"\n!pip install -U -q paddlepaddle ivy accelerate>=0.21.0  2>/dev/null # If ran in a notebook with only cpu enabled, edit \"paddlepaddle-gpu\" to \"paddlepaddle\"","metadata":{"id":"7DMn3EoEUBGQ","outputId":"357f988a-3c36-4b74-a10d-ff233047b17c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next: import the ivy library and get the dataset.","metadata":{"id":"y1sA3gFuWjDE"}},{"cell_type":"code","source":"import ivy","metadata":{"id":"_NUgteS_Dluc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import the libraries suggested in the model which is to be transpiled.","metadata":{"id":"UKN-VX8QXDEG"}},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd  # For data manipulation and analysis\nimport gc  # For garbage collection to manage memory\nimport re  # For regular expressions\nimport numpy as np  # For numerical operations and arrays\nimport tensorflow as tf\nimport torch  # PyTorch library for deep learning\nimport paddle","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nimport ivy.functional.frontends.paddle as paddle_frontend\n\n# Libraries to accompany torch's transformers\nimport tqdm\nimport boto3\nimport requests\nimport regex\nimport sentencepiece\nimport sacremoses\n\nimport warnings  # For handling warnings\nwarnings.filterwarnings(\"ignore\")  # Ignore warning messages\n\nfrom transformers import AutoModel, AutoTokenizer  # Transformers library for natural language processing\n# from transformers import TextDataset, LineByLineTextDataset, DataCollatorForLanguageModeling, \\\n# pipeline, Trainer, TrainingArguments, DataCollatorWithPadding  # Transformers components for text processing\nfrom transformers import TextDataset, LineByLineTextDataset, DataCollatorForLanguageModeling, \\\npipeline, TrainingArguments, DataCollatorWithPadding\nfrom transformers import AutoModelForSequenceClassification  # Transformer model for sequence classification\n\nimport accelerate\n\n# from nlp import Dataset  # Import custom 'Dataset' class for natural language processing tasks\nfrom imblearn.over_sampling import RandomOverSampler  # For oversampling to handle class imbalance\n# import datasets  # Import datasets library\n# from datasets import Dataset, Image, ClassLabel  # Import custom 'Dataset', 'ClassLabel', and 'Image' classes\nfrom transformers import pipeline  # Transformers library for pipelines\nfrom bs4 import BeautifulSoup  # For parsing HTML content\n\nimport matplotlib.pyplot as plt  # For data visualization\nimport itertools  # For working with iterators\nfrom sklearn.metrics import (  # Import various metrics from scikit-learn\n    accuracy_score,  # For calculating accuracy\n    roc_auc_score,  # For ROC AUC score\n    confusion_matrix,  # For confusion matrix\n    classification_report,  # For classification report\n    f1_score  # For F1 score\n)\n\n# from datasets import load_metric  # Import load_metric function to load evaluation metrics\n\nfrom tqdm import tqdm  # For displaying progress bars\n\ntqdm.pandas()  # Enable progress bars for pandas operations","metadata":{"id":"19rgBXHJXHFu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"gpu:0\" if paddle.device.cuda.device_count() else \"cpu\" # Either \"gpu\" or \"gpu:0\".\nivy.set_default_device(device)\nivy.set_soft_device_mode(True)\n","metadata":{"id":"bXr9tGFLGRPI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(ivy.default_device())\nprint(ivy.num_gpus())\nprint(torch.cuda.is_available())","metadata":{"id":"ijs6fSKL9QZ4","outputId":"23f84802-981c-4869-ea4c-c9a94489e2ff","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Set the seeds.","metadata":{"id":"JU7qbxYdsVlK"}},{"cell_type":"code","source":"tf.keras.utils.set_random_seed(0)\ntorch.manual_seed(0)\npaddle.seed(0)","metadata":{"id":"HxD1xridsU_l","outputId":"1b43d30a-d4bb-401b-f540-988ccbc9b24a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get the API key for ivy transpiler from your account and upload it to the project. Move it to the correct directory.","metadata":{"id":"zwU4oNrkXyxT"}},{"cell_type":"code","source":"pwd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First we're loading the tokenizer and the model from torch. All of the basic set-up instructions can be found here: https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/huggingface_pytorch-transformers.ipynb#scrollTo=72d8f2de","metadata":{"id":"DWCnfosUshGK"}},{"cell_type":"code","source":"tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\n","metadata":{"id":"2rZY3rhisgXZ","outputId":"6737a6a8-76a2-4881-dcaa-1e2fa12372b4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ivy.stateful.module import Module\nfrom ivy.stateful.sequential import Sequential\nfrom ivy.stateful.layers import *\nfrom ivy.stateful.losses import *\nfrom ivy.stateful.optimizers import *\nfrom ivy.stateful.activations import *\nfrom ivy.stateful.initializers import *\nfrom ivy.stateful.norms import *\n","metadata":{"id":"b0MdcERZYS_6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = pd.read_csv(\"/content/demos/Contributor_demos/Sarcasm Detection/train-balanced-sarcasm.csv\")\ndf = pd.read_csv(\"/kaggle/input/sarcasm/train-balanced-sarcasm.csv\")\ndf = df.drop_duplicates()\ndf = df.rename(columns={'comment': 'title'})\ndf = df[['label', 'title']]\ndf = df[~df['label'].isnull()]\ndf = df[~df['title'].isnull()]\ndf.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATASET AND MODEL OVERVIEW","metadata":{"id":"bXFPiT6SgPob"}},{"cell_type":"code","source":"!echo -n API_KEY > .ivy/key.pem","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_words(text: str) -> int:\n  return len(text.split())\n\ndef count_symbols(text: str) -> int:\n  return len(\"\".join(text.split()))\n\ndef symbol_to_word_ratio(text: str) -> float:\n  return count_symbols(text)/count_words(text)\n\ndef upper_lower_ratio(text: str) -> float:\n  text = \"\".join(text.split())\n  return sum(1 for c in text if c.isupper())/(max([sum(1 for c in text if c.islower()), 1]))\n\ndf['word_count'] = df[\"title\"].apply(count_words)\ndf['symbol_count'] = df[\"title\"].apply(count_symbols)\ndf[\"upper_lower_ratio\"] = df[\"title\"].apply(upper_lower_ratio)\ndf[\"symbol_to_word_ratio\"] = df[\"title\"].apply(symbol_to_word_ratio)\ndf.sample(5)","metadata":{"id":"BVA6U5Y0c7vg","outputId":"13606496-e191-4005-a04a-4e516875bdfa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A few plots to see some some characteristics of the data.","metadata":{"id":"YY9ru5DbJqxa"}},{"cell_type":"code","source":"df_no_sarc = df.where(df[\"label\"] == 0)\ndf_no_sarc = df_no_sarc.where(df_no_sarc[\"word_count\"] <= 51)\ndf_sarc = df.where(df[\"label\"] == 1)\ndf_sarc = df_sarc.where(df_sarc[\"word_count\"] <= 51)\ndf_no_sarc = df_no_sarc[np.isfinite(df_no_sarc[\"word_count\"])]\ndf_sarc = df_sarc[np.isfinite(df_sarc[\"word_count\"])]\nplt.style.use('_mpl-gallery-nogrid')\n\nhist_df_no_sarc, bin_edges_no = np.histogram(df_no_sarc[\"word_count\"].values, density=True)\nhist_df_sarc, bin_edges = np.histogram(df_sarc[\"word_count\"].values, density=True)\n# plot:\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nbin_mids_no = [(bin_edges_no[i+1] + bin_edges_no[i])/2 for i in range(len(bin_edges_no) - 1)]\nbin_mids = [(bin_edges[i+1] + bin_edges[i])/2 for i in range(len(bin_edges) - 1)]\nax1.bar(bin_mids_no, hist_df_no_sarc, width=bin_edges_no[1] - bin_edges_no[0])\nax2.bar(bin_mids, hist_df_sarc, width=bin_edges[1] - bin_edges[0])\nax1.set_title(\"Hist no sarcasm\")\nax1.set_ylabel(\"density\")\nax1.set_xlabel(\"word count\")\nax1.set_xticks(bin_edges_no)\nax1.grid(True)\nax2.set_title(\"Hist sarcasm\")\nax2.set_xlabel(\"word count\")\nax2.set_xticks(bin_edges)\nax2.grid(True)\nplt.show()","metadata":{"id":"_LEHSFedgIBq","outputId":"f6f7cb26-7383-4fa9-f454-b7f07adba063","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_no_sarc = df.where(df[\"label\"] == 0)\ndf_no_sarc = df_no_sarc.where(df_no_sarc[\"symbol_count\"] <= 201)\ndf_sarc = df.where(df[\"label\"] == 1)\ndf_sarc = df_sarc.where(df_sarc[\"symbol_count\"] <= 201)\ndf_no_sarc = df_no_sarc[np.isfinite(df_no_sarc[\"symbol_count\"])]\ndf_sarc = df_sarc[np.isfinite(df_sarc[\"symbol_count\"])]\nplt.style.use('_mpl-gallery-nogrid')\n\nhist_df_no_sarc, bin_edges_no = np.histogram(df_no_sarc[\"symbol_count\"].values, density=True)\nhist_df_sarc, bin_edges = np.histogram(df_sarc[\"symbol_count\"].values, density=True)\n# plot:\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nbin_mids_no = [(bin_edges_no[i+1] + bin_edges_no[i])/2 for i in range(len(bin_edges_no) - 1)]\nbin_mids = [(bin_edges[i+1] + bin_edges[i])/2 for i in range(len(bin_edges) - 1)]\nax1.bar(bin_mids_no, hist_df_no_sarc, width=bin_edges_no[1] - bin_edges_no[0])\nax2.bar(bin_mids, hist_df_sarc, width=bin_edges[1] - bin_edges[0])\nax1.set_title(\"Hist no sarcasm\")\nax1.set_ylabel(\"density\")\nax1.set_xlabel(\"symbol count\")\nax1.set_xticks(bin_edges_no)\nax1.grid(True)\nax2.set_title(\"Hist sarcasm\")\nax2.set_xlabel(\"symbol count\")\nax2.set_xticks(bin_edges)\nax2.grid(True)\nplt.show()","metadata":{"id":"RcYhYzfygLc9","outputId":"df9ed393-4d87-4431-b393-76729e77cb97","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_no_sarc = df.where(df[\"label\"] == 0)\ndf_no_sarc = df_no_sarc.where(df_no_sarc[\"upper_lower_ratio\"] <= 0.3)\ndf_sarc = df.where(df[\"label\"] == 1)\ndf_sarc = df_sarc.where(df_sarc[\"upper_lower_ratio\"] <= 0.3)\ndf_no_sarc = df_no_sarc[np.isfinite(df_no_sarc[\"upper_lower_ratio\"])]\ndf_sarc = df_sarc[np.isfinite(df_sarc[\"upper_lower_ratio\"])]\nplt.style.use('_mpl-gallery-nogrid')\n\nhist_df_no_sarc, bin_edges_no = np.histogram(df_no_sarc[\"upper_lower_ratio\"].values, density=True)\nhist_df_sarc, bin_edges = np.histogram(df_sarc[\"upper_lower_ratio\"].values, density=True)\n# plot:\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nbin_mids_no = [(bin_edges_no[i+1] + bin_edges_no[i])/2 for i in range(len(bin_edges_no) - 1)]\nbin_mids = [(bin_edges[i+1] + bin_edges[i])/2 for i in range(len(bin_edges) - 1)]\nax1.bar(bin_mids_no, hist_df_no_sarc, width=bin_edges_no[1] - bin_edges_no[0])\nax2.bar(bin_mids, hist_df_sarc, width=bin_edges[1] - bin_edges[0])\nax1.set_title(\"Hist no sarcasm\")\nax1.set_ylabel(\"density\")\nax1.set_xlabel(\"upper/lower ratio\")\nax1.set_xticks(bin_edges_no)\nax1.grid(True)\nax2.set_title(\"Hist sarcasm\")\nax2.set_xlabel(\"upper/lower ratio\")\nax2.set_xticks(bin_edges)\nax2.grid(True)\nplt.show()","metadata":{"id":"vvKbuhLaDaSP","outputId":"f7756960-1a3d-4ace-b6a0-8addf8b6e53c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_no_sarc = df.where(df[\"label\"] == 0)\ndf_no_sarc = df_no_sarc.where(df_no_sarc[\"symbol_to_word_ratio\"] <= 11)\ndf_sarc = df.where(df[\"label\"] == 1)\ndf_sarc = df_sarc.where(df_sarc[\"symbol_to_word_ratio\"] <= 11)\ndf_no_sarc = df_no_sarc[np.isfinite(df_no_sarc[\"symbol_to_word_ratio\"])]\ndf_sarc = df_sarc[np.isfinite(df_sarc[\"symbol_to_word_ratio\"])]\nplt.style.use('_mpl-gallery-nogrid')\n\nhist_df_no_sarc, bin_edges_no = np.histogram(df_no_sarc[\"symbol_to_word_ratio\"].values, density=True)\nhist_df_sarc, bin_edges = np.histogram(df_sarc[\"symbol_to_word_ratio\"].values, density=True)\n# plot:\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nbin_mids_no = [(bin_edges_no[i+1] + bin_edges_no[i])/2 for i in range(len(bin_edges_no) - 1)]\nbin_mids = [(bin_edges[i+1] + bin_edges[i])/2 for i in range(len(bin_edges) - 1)]\nax1.bar(bin_mids_no, hist_df_no_sarc, width=bin_edges_no[1] - bin_edges_no[0])\nax2.bar(bin_mids, hist_df_sarc, width=bin_edges[1] - bin_edges[0])\nax1.set_title(\"Hist no sarcasm\")\nax1.set_ylabel(\"density\")\nax1.set_xlabel(\"symbols/words ratio\")\nax1.set_xticks(bin_edges_no)\nax1.grid(True)\nax2.set_title(\"Hist sarcasm\")\nax2.set_xlabel(\"symbols/words ratio\")\nax2.set_xticks(bin_edges)\nax2.grid(True)\nplt.show()","metadata":{"id":"HkuIzb1JF1U1","outputId":"7ab4df1a-bd8e-4188-d65b-f7d7c5075c17","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"id":"nk3vK1u3FlZc","outputId":"75d850b4-d36e-4f97-984c-92ef21501d3a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BUILDING LSTM ON CORE IVY","metadata":{"id":"Vy7iZ8QnAS2-"}},{"cell_type":"code","source":"# dir(tokenizer)","metadata":{"id":"SEPdRlMT6Ybd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Setting up the device for the computations.","metadata":{}},{"cell_type":"code","source":"print(torch.cuda.is_available())\ndevice = ivy.as_native_dev(\"gpu:0\")\nivy.set_default_device(\"gpu:0\")\nprint(ivy.default_device())\nivy.set_soft_device_mode(True)\nprint(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.vocab_size)\nprint(tokenizer.all_special_tokens_extended)\nprint(tokenizer.all_special_ids)\nprint(tokenizer.pad_token_id)","metadata":{"id":"mKyWQmUR6grC","outputId":"72aaf16b-1d93-46ee-d305-529cdf4982ae","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = list(df.sample(8)[\"title\"])\nprint(sample)\ntokenizer(sample, add_special_tokens=True, padding=True, truncation=True)","metadata":{"id":"_jEnXjjUOo7f","outputId":"3b7e5446-cd75-4c2e-d53b-3bad6852d87b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ivy.set_backend(\"torch\")\nnum_embeddings = tokenizer.vocab_size\nembedding_dim = 5\npad_token_id = tokenizer.pad_token_id\ninput_channels = embedding_dim\nnum_classes = 2\noutput_channels = 1\nnum_layers = 1\nlinear_input_channels = 2\nmax_length = 29\ntokenizer.model_max_length = max_length\neps = 1e-05\ntesting_input = list(df.sample(8)[\"title\"])\nbatch_size = 8\nlinear_input_channels = (tokenizer.model_max_length + 3) * batch_size # 3 comes from the hidden states of the LSTM\nlinear_output_channels = num_classes * batch_size\nnormalized_shape = (num_classes)\n\nclass LSTM_postproc(Module):\n\n  def __init__(self):\n    super(LSTM_postproc, self).__init__()\n\n  def __call__(self, args):\n\n    lstm_output, lstm_state = args\n    lstm_state_latest, lstm_state_hidden = lstm_state\n    lstm_state_latest = ivy.array(lstm_state_latest)\n    # print(lstm_state_hidden, lstm_state_latest)\n    lstm_state_hidden = ivy.array([state for state in lstm_state_hidden][0])\n\n    lstm_state = ivy.concat((lstm_state_latest, lstm_state_hidden), axis=0).reshape((batch_size, -1, 1))\n    # print(lstm_output.shape, lstm_state.shape)\n    out = ivy.concat([lstm_output, lstm_state], axis=1)\n    out = out.flatten()\n    return out\n\nclass Tokenizer(Module):\n\n  def __init__(self, tokenizer):\n    super(Tokenizer, self).__init__()\n    self.tokenizer = tokenizer\n\n  def __call__(self, args):\n    args = list(args)\n    return self.tokenizer(args, add_special_tokens=True, max_length=max_length, padding=\"max_length\", truncation=True)[\"input_ids\"]\n\nclass Reshaper(Module):\n\n  def __init__(self):\n    super(Reshaper, self).__init__()\n\n  def __call__(self, args):\n    return args.reshape((batch_size, num_classes))\n\nivy_LSTM = Sequential(\n    Tokenizer(tokenizer),\n    Embedding(num_embeddings, embedding_dim, pad_token_id),\n    LSTM(input_channels, output_channels, num_layers=1, return_sequence=True, return_state=True, device=None, v=None, dtype=None),\n    LSTM_postproc(),\n    Linear(linear_input_channels, linear_output_channels, with_bias=True),\n    Reshaper(),\n    Sigmoid(),\n    Softmax(),\n)","metadata":{"id":"d_uPnXJd7flG","outputId":"179d3094-7c3a-4cbd-b05b-0fcfc18c306a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dir(ivy_LSTM))\nprint(ivy_LSTM.device)","metadata":{"id":"7Igom39EGNt8","outputId":"ff871a1e-4557-4024-c3c1-353f4ad3accd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ivy_Dataset(Dataset):\n    def __init__(self, df):\n      self.num_samples = df['title'].size\n      self.data = [[entry[0], entry[1]] for entry in zip(df[\"title\"], df[\"label\"])]\n\n    def __getitem__(self, idx):\n        title = self.data[idx][0]\n        label = self.data[idx][1]\n        return title, label\n\n    def __len__(self):\n        return self.num_samples\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data = ivy_Dataset(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nbatch_size = 8\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ivy_train_loader(dataset = df, batch_size = 8):\n  num_batches = int(len(dataset)/batch_size)\n  out = ((dataset[\"title\"][batch_idx * batch_size : batch_idx * batch_size + batch_size], dataset[\"label\"][batch_idx * batch_size : batch_idx * batch_size + batch_size]) for batch_idx in range(num_batches))\n  return out\n\nloader = ivy_train_loader()\nfor batch_id, data in tqdm(enumerate(loader)):\n    x_data = data[0]\n    y_data = data[1]\n    ivy_LSTM_test_out = ivy_LSTM(x_data)\n    # print()\n    # print(ivy.sum(ivy_LSTM_test_out, axis=1))\n    if batch_id == 4:\n      break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch_id, data in tqdm(enumerate(train_dataloader)):\n    x_data = data[0]\n    y_data = data[1]\n    ivy_LSTM_test_out = ivy_LSTM(x_data)\n    if batch_id == 4:\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_hot(args, num_clases = 2):\n  out = [[1 if idx == elem else 0 for idx in range(2)] for elem in args]\n  return out\n\ndef argmax(args):\n  out = [ivy.argmax(elem) for elem in args]\n  return out\n\nprint(one_hot([0, 0, 1, 0]))\nprint(argmax(ivy.array([[0.49967843, 0.50032151],\n       [0.49986687, 0.50013322],\n       [0.49912587, 0.50087422],\n       [0.50080854, 0.4991914 ],\n       [0.50049627, 0.4995037 ],\n       [0.4998956 , 0.50010443],\n       [0.50008798, 0.49991205],\n       [0.50053447, 0.49946556]])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ivy.set_backend(\"torch\")\nnum_embeddings = tokenizer.vocab_size\nembedding_dim = 3\npad_token_id = tokenizer.pad_token_id\ninput_channels = embedding_dim\nnum_classes = 2\noutput_channels = 1\nnum_layers = 1\nlinear_input_channels = 2\nmax_length = 13\ntokenizer.model_max_length = max_length\neps = 1e-05\ntesting_input = list(df.sample(8)[\"title\"])\nbatch_size = 8\nlinear_input_channels = (tokenizer.model_max_length + 3) * batch_size # 3 comes from the hidden states of the LSTM\nlinear_output_channels = num_classes * batch_size\nnormalized_shape = (num_classes)\n\nclass LSTM_postproc(Module):\n\n  def __init__(self):\n    super(LSTM_postproc, self).__init__()\n\n  def __call__(self, args):\n\n    lstm_output, lstm_state = args\n    lstm_state_latest, lstm_state_hidden = lstm_state\n    lstm_state_latest = ivy.array(lstm_state_latest)\n    # print(lstm_state_hidden, lstm_state_latest)\n    lstm_state_hidden = ivy.array([state for state in lstm_state_hidden][0])\n\n    lstm_state = ivy.concat((lstm_state_latest, lstm_state_hidden), axis=0).reshape((batch_size, -1, 1))\n    # print(lstm_output.shape, lstm_state.shape)\n    out = ivy.concat([lstm_output, lstm_state], axis=1)\n    out = out.flatten()\n    return out\n\nclass Tokenizer(Module):\n\n  def __init__(self, tokenizer):\n    super(Tokenizer, self).__init__()\n    self.tokenizer = tokenizer\n\n  def __call__(self, args):\n    args = list(args)\n    return self.tokenizer(args, add_special_tokens=True, max_length=max_length, padding=\"max_length\", truncation=True)[\"input_ids\"]\n\nclass Reshaper(Module):\n\n  def __init__(self):\n    super(Reshaper, self).__init__()\n\n  def __call__(self, args):\n    return args.reshape((batch_size, num_classes))\n\nclass Argmax(Module):\n\n  def __init__(self):\n    super(Argmax, self).__init__()\n\n  def __call__(self, args):\n    return ivy.argmax(args, axis=-1)\n\n\n\nivy_LSTM = Sequential(\n    Tokenizer(tokenizer),\n    Embedding(num_embeddings, embedding_dim, pad_token_id),\n    LSTM(input_channels, output_channels, num_layers=1, return_sequence=True, return_state=True, device=None, v=None, dtype=None),\n    LSTM_postproc(),\n    Linear(linear_input_channels, linear_output_channels, with_bias=True),\n    Reshaper(),\n    Sigmoid(),\n    Softmax(),\n    Argmax(),\n)","metadata":{"id":"4dQ2Recq9Xo8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_ivy(model):\n  logs = []\n  learning_rate = 3e-5\n  opt = SGD(lr=learning_rate, inplace=True, stop_gradients=True, trace_on_next_step=False)\n  loss_fn = CrossEntropyLoss(axis=-1, epsilon=1e-07, reduction='sum')\n  epochs = 2\n  grads = ivy.zeros_like(model.v)\n  classifier = model\n  train_loader = ivy_train_loader(df, batch_size)\n\n  for epoch in range(epochs):\n\n    for batch_id, data in tqdm(enumerate(train_loader)):\n\n      x_data = data[0]\n      y_data = list(data[1])\n      # print(y_data)\n      # The transpiled model seems to have problems with inputs, so instead of feeding it a container, we map onto one.\n      predictions = classifier(x_data)\n\n      loss = loss_fn(predictions, y_data).float()\n      loss.requires_grad = True\n      # print(f\"LOSS: {loss}\")\n      \n      # acc = paddle.metric.accuracy(predicts, y_data) # This needs to be corrected.\n      loss.backward()\n\n      # update parameters\n      opt.step(model.v, grads)\n\n      if batch_id % 100 == 0:\n          # print(\"\\nepoch: {}, batch_id: {}, loss is: {}, acc is: {}\".format(epoch, batch_id, loss.numpy(), acc))\n          logs.append([epoch, batch_id, loss])\n\n      # opt.clear_grad()\n    gc.collect()\n\n\n  obj = {'model': model.state_dict(), 'opt': opt.state_dict(), 'epoch': epochs}\n  path = '/kaggle/working/demos/Contributor_demos/Sarcasm Detection/model.pdparams'\n  paddle.save(obj, path)\n\n  return logs, model","metadata":{"id":"1NqBJx51TYSd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logs, model = train_ivy(ivy_LSTM)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}